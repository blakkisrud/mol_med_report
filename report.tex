\documentclass[12p]{article}       
\usepackage[utf8]{inputenc}
\usepackage{parskip}
\usepackage{graphicx}       				% For å inkludere figurer.
\usepackage{amsmath,amssymb} 				% Ekstra matematikkfunksjoner.
\usepackage{siunitx}% Må inkluderes for blant annet å få tilgang til kommandoen \SI (korrekte måltall med enheter)
\usepackage{float}
\usepackage{geometry}


\def\ps@pprintTitle{%
\let\@oddhead\@empty
\let\@evenhead\@empty
\def\@oddfoot{}%
\let\@evenfoot\@oddfoot}

%	\sisetup{exponent-product = \cdot}      	% Prikk som multiplikasjonstegn (i steden for kryss).
% 	\sisetup{output-decimal-marker  =  {,}} 	% Komma som desimalskilletegn (i steden for punktum).
 %	\sisetup{separate-uncertainty = true}   	% Pluss-minus-form på usikkerhet (i steden for parentes). 
\usepackage{booktabs}                     		% For å få tilgang til finere linjer (til bruk i tabeller og slikt).
\usepackage[font=small,labelfont=bf]{caption}		% For justering av figurtekst og tabelltekst.

% Disse kommandoene kan gjøre det enklere for LaTeX å plassere figurer og tabeller der du ønsker.
\setcounter{totalnumber}{5}
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.35}
\geometry{margin = 0.75in}

\title{A summary of lectures given in the subject MF9120 - written in all verb tenses from my lecture notes}
\author{Johan Blakkisrud}

\begin{document}

\maketitle

\section*{Monday 14. Nov}

\subsection*{Session 1 - Introduction to animal models and gene editing}
The main reason to use animal models is to exploit the similarities between humans and other organisms, thus preventing the use of human test subjects.
The similarities is maybe most present when embryos from different animals are compared, even though the adult individual does not look the same at all, the embryoes are nearly identical.
It is a major step up from cell-like studies, as a whole organism can be used as a model.
Animal welfare is very important, the three R's (replace, refine and reduce) is a nice rule of thumb.
The animals range a wide variety of complexity, from the simple flatworm (Caenorhabidtis Elegans) to the more human-like rodent, the rat.

The most "classic" animal, the rat, is a relatively human-like organism.
They have a generally short life-span, speeding up experiments.
However, the life span of rats in an experimental setting is considered long, when compared to flat-worm or fruit-flies.
Recently, apparently less human-like animals have been used, as the flat-worm and zebra-fish.
These animals do not look like humans, but we share a great deal of cellular and molecular path-ways.
This, combined with small size and short life-spans make them excellent research animals in molecular and genetic studies.

Gene editing has become a particularly powerful tool when investigating diseases.
The work-flow can for example start with a discovered gene attributed to a human disease (say an epidemiologist recognize a particular trait in closely related individuals).
The task at hand then becomes to implement the discovered gene in an animal model, either by knocking out the particular gene, over express it or a combination of these.
It can also go the other way around, when scientists discover a mutation in a particular animal model that lead to a disease in the animal that resembles a human illness.

Roughly, three types of genetic animal models exist:

\begin{itemize}
\item
    Knock-out:These are the more extreme cases. 
    The gene that is studied in the animal model is completely knocked out.
\item
    Transgenic: Here, a normal gene is substituted with a mutated variety of said gene.
    This could be considered less extreme as it doesnt neccecary involve development of the animal.
\item
    Conditional knock-out: Highly specific knock-out allow us to delete a gene in a specific tissue.
    This allow us to develop highly tailored animal models.
\end{itemize}

The tool that allow us to develop these animal models have evolved during the past years, with the introduction of the CRISPR/CAS9 technique as the main contributor.
The technique is encountered everywhere in genetic research, and put short, it allow us to introduce highly specific DNA-double strand breaks.
These breaks further allow to substitute, or completely cut out a gene expression with a high degree of specificity.

\subsection*{Session 2 - High output screening and the zebra fish}

\subsubsection*{High output screening}

High output screening (HTS) is a highly advanced, highly useful technology. 
In short, it is the classical biochemical test, combine compounds in a well and observe the outcome, on an industrial scale.
This is achieved with robotics and a high degree of automation.
The HTS system consist of multiple parts, main system parts being the assay plate, robotic handling, compound storage, detection and data integrity.
The assay plate should be standardized and fit to the other compartments; a much used format is the 384 well plate.
The robot handling is an industrial robot that can transport and store the assay plates. 
A critical part of the handling process is the liquid handling - now highly refined techniques can use mechanical (sound) waves to dispense volumes down to 2.5 nl.
The various compounds is stored in "libraries" that can stretch from a couple of hundreds to several millions of different compounds.
This storage aspect means that a great deal of effort is used towards storing, to avoid degradation.

\subsubsection*{The Zebra-fish}

The zebra-fish was mentioned in the chapter of animal models, but was more thorough presented in its own session, and that it highly deserves.
The zebra-fish is a vertebrate with highly genetic and physiological similarities with humans.
It develops very rapidly, with a gestaltion time of a few days.
Here also lies one of the greatest attributes of the zebra-fish - the development is ex-utero, with a transparent infant stage.
This means we can follow the development of the individual in a very direct and nice manner, making it an excellent model for studying fetal development.
Due to its small size, relatively small amount of compounds are needed, and the zebra-fish absorb it directly through skin and gills.
This means we can place the fish (either adults or more likely, embyonic) in an environment containing the compound, and thats all that is needed.
This can be combined by high output screening, allowing large volumes of experiments.
Even though the zebra-fish looks simple, it can be used for quite "high-level" diseases involving complex neurological illnesses like epilepsy.

\subsection*{Session 3 - Pharmacology and the G-protein coupled receptors}

Pharmacology is a large field that tries to answer a set of questions, mainly: How can a drug be \emph{effective} and \emph{selective}?
For the drug to be selective, it is meant that it acts on the desired target, and preferably that target alone.
The optimum scenario is naturally that the drug only works on the selected target, but this ideal is seldom met, and the reality is a trade-off between the optimal therapeuic effect and the minimal side-effects
Drug targets are classified into various classes, large ones being receptors, ion-channels, transporters and enzymes. 
These classes sometimes overlap.
The receptors have their own subclasses, common for of all them is that the primary task is to mediate some kind of signal.
This signal is passed on by the receptor recognizing some transmitter (a ligand) and undergo some chemical change that has an effect.
A large group of receptors are the G-protein receptors.
The G-protein receptors contains several sub-units, allowing complex trans-membrane signalling.
Classification of all the types of G-protein receptors is a field of its own, they can be classified by ligand, which effector system they utilize; or maybe a genetic classification is the best?
Drugs can work as both antagonist (blockers) and agonist (imitating the ligand).
Drugs that are associated with the G-protein receptors are various adreenergetics, chelinenergics and serotonin and histamine analogs.

The catalytic transmembrane proteins spans the membrane and the classical example is the receptor tyrosine kinases.
The ligand lead to a phosphorylation, changing the structure of the protein on the inside of the cell membrane, and mediates the signal.
Ligand gated ion channels goes into the realm of ion channels, but a chemical ligand is needed to activate (or de active) it.
Some steroid hormones work on these ligand-gated ion channels, vitamin D, vitamin A and several thyroid hormones.

The other large receptor group are the ion channels, serving as a gate-keeper for ionic potential  across the cell membrane.
These could be voltage gated (relying on an incoming depolarization) or mechanical-working, relying on a complex system of proteins.
Ion-channels ventures into the realm of transporters, which facilitate transport across the cell membrane, either biomolecules, ions or water.
A classic transporter is the synaptic nerve system, and inhibitors of these transporters are the basis for many antidepressants.
Enzymes can also acts as drug targets, changing the structures and render them non functioning.

When looking at dose-effect relationship, one can encounter a quite straight forward mathematical mode, where the effect of the drug goes as a sigmoid of the dose (reaching a plateau where the effect is saturated).
The half-way point is often a meassure for dose-rate effect, the dose to reach half the saturated effect.
However, more complicated situations can arise when a mix of agonist and neutral antagonists are present, or, if the ligand acts via multiple receptors, making the picture quite complicated.

\section*{Tuesday 15. Nov}

\subsection*{Session 1 - Inflammation and pluripotent stem-cells}

Inflammation. 
Interestingly, inflammation is not neccecary an infection, the infection is the \emph{cause} of the inflamtion.
It is one of the bodies most quick and efficient protective response.
The main "job" of the inflammation process can be summarized quite nicely in the "five Rs of inflammation":

\begin{enumerate}
    \item
	Recognition 
    \item
	Recruitment 
    \item
	Removal
    \item
	Regulation
    \item
	Resolution
\end{enumerate}

The first step is recognition, a foreign object must be identified.
The situation with the foreign object is the most "classical" scenario when we think inflammation, but it does not have to be an element from "outside".
Damaged tissue can also initiate an inflammation process, either caused by trauma or ischemia.
The foreign object does not need to be a bacteria or virus, it could also be silica crystals or other non-biotic elements.
Inflammation can also be triggered by an immune reaction due to allergies or body transplants.

When the inflammation is initiated, some macroscopic symptoms can be identified.
These symptoms has been described since ancient times, and includes heat, redness, swelling, pain and less of function.
The pain is maybe the less "intuitive" symptom, as it is little to do when the infection occurs, however, it helps to "immobilize" so the body gets much needed rest.

The microscopic scene is fairly complex and involves several cellular participants, but the main early mechanism is to recruit cells to deal with the damage. 
In short for the innate immune system, an important task is to recruit leukocytes from the blood stream into the tissue.
This is done by expressing P and E-selectin on the surface of the endothelium, these "selects" the leukocytes flowing past, attaching and passes the membrane.
When in the tissue, chemokines, which recruit more leukocytes.
Sometimes a "compliment molecule is also needed.
When the leukocyte is activated, and the microbe adheres to it, the microbe meets its fate getting killed by reactive oxygen.

\subsubsection*{Pluripotent stem-cells}

The talk was about stem-cells, introducing some concepts, possibilities, nomenclature and examples.
There are two kinds of stem cells, embryonic stem cells and adult stem cells.
The embryonic kind can divide indefinitely and can differentiate to every type of cell.
Adult stem cells are more limited, they can regenerate certain kinds of tissue and are much more difficult to culture in vitro.

The potential of stem cells can be divided into totipotent (the zygote that can give rise to an embryo), the pluripotent and the multipotent.
All organisms generally start as a totipotent cell, and further and further differentation limits the specific cell potential, down to a unipotent cell.
This can be though of as a process where genes are sequentially turned off, it is then naturally a riveting though to how these cells can be "turned on" again and get back to their "high potential state".
A set of transcription regulators have been identified with this process, Oct4, Sox2 and Nanog.
To maintain pluripotency, Oct4 and Sox2 are needed, and Nano acts as a safeguard for the pluripotency.

Embryonic stem cells are the ones with the greatest potential, they can be, if held in their proper state, used to produce all the different cells of the body.
The need for fertilized eggs do naturally bring on some ethical concerns.
It is therefore tempting to explore the possibilities to take a differentiated cell, grow it together with the aforementioned pluripotent state proteins, and get induced pluripotent stem cells, which in turn can be differentiated into the desired tissue cells.

As we then can harvest cells from a donor, and use them to make tissue- and patient specific, a powerful tool in personalized medicine is acquired.
Attempts are promising but are riddled with challenges, the biggest being sub-par and unoptimized differentation protocols and circumvent problems induced by epi-genics when recreating an adequate cell culture.
However, if we overcome these obstacles we can grow organs meeting the transplant need with patient specific tissue, test for patient specific toxicity and numerous other application when patient specific tissue samples are needed in larger quanta.

\subsection*{Session 2 - Molecular pathgogenesis and tumor angiogenesis}

The talk was about the hallmarks of cancer, with focus on angiogenesis.
Cancer is a large disease in terms of affected number of individuals.
The incidence has risen steadily for both male and female patients, except for stomach cancer.
The increase in incidence comes to some extent from life-style changes, but also from screening and earlier detection of malignancies that some years ago would go on undetected.
A neoplasm, or tumor, is an uncontrolled growth of tissue that invades and spreads to other tissue.
In stained tissue samples they tend to look star-shaped and more irregular than their non-tumorous counterparts.
The tumor has a characteristic micro environment, with different cells making up a fairly complex environment.

The characteristics of cancer, or apply called the "hallmarks of cancer" was coined and published in a seminal work by Hanahan and Weinberg in 2000.
They there described six hallmarks.
This was revised in 2011 to include additionally four, ending in a total of ten hallmarks.
These hallmarks are here described:

\begin{enumerate}
  \item
    Sustaining proliferative signaling:
    The body has mechanisms to avoid proliferation without limits.
    These mechanisms are among other factors, a series of molecular signals, hindering the cell to divide.
    When this hormonal system is tampered by maintaining a constant "positive" signalling for proliferation, neoplasms can occur.
  \item
    Evading tumor growth suppressors:
    It not important just to maintain a positive growth, but to hinder "negative growth signals" in the form of regulator proteins as p16-RB, p21 and p53.
    Specifically, as shown by a study from 2002, patients with melanomas that do not express p16 have a much worse survival out-come.
  \item
    Activating invasion and metastasis:
    The tumor tissue, in order to spread, must have an ability to break through the epithelium and wander around in the organism.
    This is done partly by the production of proteases that degrade the basal membrane, or through so called epithelial-mesenchymal transition(EMT).
    EMT is a process, starting with the loss of E-Cadherin, that in turn makes the basal membrane loosing its cell to cell adhesion capabilities, allowing cancer cells to move through and into the blood stream.
  \item
    Enabling replicative immortality
    Tumor cells have a unique capability to sustain "indefinitely" given enough nutrients.
    This is achieved by tampering with the check-point mechanisms that ultimately kills cells when they have reached a certain number of divisions, the telomers.
  \item
    Inducing angiogenesis:
    Uncontrolled growth without a proper blood-supply is difficult.
    Therefore, the tumors has to induce growth of the blood-supply as well.
    The previous there was the tumors got some infiltration of blood vessels, and then expanded the size of these vessels.
    A new understanding is that the blood vessels in dead reach further into the tumor as the tumor is expressing angiogenic promotion proteins.
    These proteins are in normal functioning tissue associated with wound healing, and thus the cancer can be though of as a wound that never heals, taking advantage of this situation.
    The proteins are various and numerous, but the most classical ones are the "positive" VEGF, bFGF, but controller do also exist and are associated with angiogenesis.
    A third factor is bone marrow involvement, where haemapoetic stem cells are recruited to differentiate into endothelial stem cells.
  \item
    Resisting cell death:
    Tumors are foreign tissue and should under normal circumstances be cleared out by ether apoptosis, necrosis or autophagy, preferably in a controlled fashion (apoptosis and autophagy)
  \item
    Deregulating cellular energetics:
    It is believed that tumors have different metabolism than normal tissue, the most extreme being a metabolism that exclusively relies on glycolysis (The Warburg-effect)
  \item
    Avoiding immune destruction:
    The immune system have ways of destroying cancer tissue if they can be recognized as foreign.
    The tumor tissue can fool the immune system by expressing (or not express) surface proteins that mask the malignancy for the immune system.

\end{enumerate}
    
The two remaining hallmarks are "Tumor promotion inflammation" and "Genome instability and mutation"

\subsection*{Session 3 - Breast cancer}

The talk was about breast cancer genomics, describing the work flow from risk loci to cancer phenotypes.
Breast cancer is the most common disease in women, just in Norway there are 2 700 new cases each year, consisting of 25 \% of the total number of cancers.
Breast cancer can be classified using expression arrays into multiple subtypes.
These subtypes are called molecular intrinsic subtypes.
It has been shown that these subtypes stratifies the patients in regards to survival probability, with some differences being quite dramatic.
It can be shown that different tumor tissue express ER, PR and HER2 different, and treatment can be targeted towards these, inhibiting transcription of the proteins.
These drugs are somewhat ineffective, and the need arise for better treatment.

A large work have now found promising new targets, when mutation in a set of genes seem to correlates with clinical features.
One of these proteins is FOXA1, a transcription factor that is expressed in luminal tumors, but seem to correlate with a positive outcome in terms of survival.
Experiments show the role of FOXA1, being necessary for estrogen induced transcription.

\section*{Wednesday 16. Nov}

\subsection*{Session 1 - Introduction to imaging modalities}

The talk was an introduction to various imaging modalities used in a clinical setting.
Broadly the imaging modalities can be classified into three categories.
1: The use of ionizing radiation from an external source that traverses the body and gets detected on the other side.
This is used to classify tissue density in the image in the case of X-ray photograpgy, when a number of projections are constructed into a volumetric image we call it CT.
The spatial resolution of this technique is comparably high, but depends on hard-ware; most modern implementation give sub mm resolution.

The second category is using ionizing radiation from tracers inside the body to reconstruct a volumetric image of the distribution of the tracer.
When this is done with a single emission photon isotope we called it SPECT, when it is done from two photons originating from an electron-positron annihilation we call it PET.
The resolution is worse than the CT (on the mm-scale) for PET, and even worse for SPECT.
A fundamental resolution problem for the PET is the random walk of the positron before annihilation.
The obvious nice thing about the emission tomography techniques is that they can image an isotope tracer that can be, if chemistry allow it, linked to any compound, and their way through the body can be visualized.
This allow us to study functional aspects of the body, differing from the much more "inherently anatomic" nature of the CT.
This is however not entirely true, as you can use contrast agents for CT as well, and with spectral-CT you can differentiate between two image weighings to image two aspects at the same time (i.e. both anatomical and physiological (in the form of a contrast agent)).
SPECT and PET are however almost useless without a hybrid CT-scan, primarily for doing attenuation correction.

The third main category is imaging using the magnetic properties of water to visualize relaxation differences in tissue.
Then it is called MR and it is a bit voodoo. 
The soft-tissue differentiation in MR is superb, and the added benefit of having no known risk of damage do to an examination (one encounters some heating of the tissue but it a minuscule amount) makes MR a great image option.
One obvious limitation is tissue density problems, you do not have the same "direct tissue density read-out" as the CT and have to calibrate your system.
The sensitive of MR compared to SPECT, PET and CT is also very low, requiring a long acquisition time.
MR can also be used to investigate the chemical composition of a region of the body (typically a large voxel) that especially for identifying chemical signaling mediators in the brain is very useful.
The use of these imaging modalities in a clinical oncology setting is described elsewhere.


\subsection*{Session 2 - Cell based therapies - Lab work and work-flow}

When developing new drugs, in a well functioning scientific community, strict regulations and documentation demand is an imperative.
This is by no means restricted to drugs involving chemicals, but reaching also cell-based therapy, where there exist a large amount of rules and regulations.
This regularities are in constant development as novel cellular therapies emerges.
Personnel, equipment, safety, standard operating procedures and storage all have their specific regulations, ensuring a safe and robust research, development and production environment.

T-cell infiltration in tumor or the stroma has been identified (for quite some time ago) as prognostic factors for therapy end-points such as recurrence free survival, progression free survival and overall survival.
T-cell in the micro-environment of the tumors significantly stratified patient groups in Non-small cell Lung cancer cohort, time to half fraction disease specific survival separated with as much as 70 months.
It is tempting then to do mRNA-profiling, identifying the immunologic signature, as it has been shown that surgery is less effective on survival with the immune system not involved - maybe pivoting to other treatments?
However, the antigen associated with these processes are largely unknown, except for some melanoma-cases.

Profiling is one thing, another is active use of immunocells in cancer therapy.
A number of different therapies are currently being investigated, some in the Department of Cellular Therapy.
Among these therapies is dendritic cell vaccines, where we extract monocytes from the patient, incubate them with the needed medium to cultivate effective dendritic cells, which then are administered into the patient.
This is a multi-step process that requires strict attention to details in making a working GMP-production.
Another treatment is adoptive cell therapy, where multipotent haemapoetic stem cells are extracted from a patient (or a donor), trained and reinfused into the patient.
The procedure is quite risky, relying on knocking out the patients immune response and serious adverse effects can arise.
T-cells can be used in same manner, extracted, cultivated and reinfused.
This cellular therapies implies a large (and shifting) amount of regulations.

\subsection*{Session 3 - Patophysiology of exicatory diseases in the brain}

Glutamate is a highly interesting biological molecule, on one hand it is a basic and abundant neutransmitter, on the other hand a very potent neurotoxin.
The transmitting capabilities of glutamate was first described by Hayashy in 1954, when convolutions were observed when glutamate was injected into the brain or cartoid arteries of mice.
This physiological transmission of glutamate is imperative for normal function of the nerve system.
The glutamate is stores in vecicles in the synaptic nerve, are released into the region between the synapses and reaches receptors on the adjacent synapse.
The glutamate are then quickly cleared out, stored shortly in the astrocyte for then to be transported back into the synaptic nerve as glutamine.
Multiple glutamate receptors exist, and can be broadly categorized into three categories, AMPA-R, KAINATE-R and NMDA-R.
Further, NMDA-R has three subtypes, and the primary source of calcium influx.
The NMDA.-recetor consist of an N and C-terminal, and four M-groups on the membrane level.
The response of the receptor is a three-step process.
First, glutamat binds to the GluN2-unit, then GluN1 binds glycin, needed to open the Ca-channel.
The last step is relase of magnesium which when bound blocks the channel.
The relase of magnesium is facilitatet by AMPA-R receptors, and thus acts as a zeroth step for glutamate activation.
A remaining point on the NMDA-receptors is the connection with the PDZ-containing scaffold protein, the PSD-95 family proteins.
These proteins facilitate the insertion and anchoring of NMDA-receptors at synaptic sites, this plays a major role for the localization of them.

Exicitotoxicity is a pathological process involving the NMDA and AMPA-receptors.
The end-point of the process is severe neuron death.
These events can be triggered by a stroke, when nerve cells around an ischemic core relases a large amount of glutamate upon death.
The glutamate flows into the ischemic penumbrae, the area around the core that is not yet severely damaged.
The transport system that clears out the glutamate is hindred, and the result is a massive overactivation of glutamate, leading to an excess and un-interupted influx of calcium.
This leads in the next step to programmet cell death (apoptosis) and further brings damage to the organism.
The details of the pathways leading to cell death from the influx of calcium is debated.
They can be categorized into two main hypothesis:

\begin{enumerate}
\item
The threshold theory, involving several proteins in separate pathways, main actors being CASPASE, PARP1 and PSD95-NOS. The excess of calcium determines the excitotoxicity.
\item
The specific route theory: More debated, a precise and highly specific (but not neccesarily critically large) amount of calcium influx cause cell death
\end{enumerate}

\section*{Thursday 17. Nov}

\subsection*{Session 1 - Imaging cardiovascular diseases in animals and patients}

Imaging of muscle-cells during response is primarily done by measuring the calcium concentration in cytosol, giving an indication of muscle activity.
This is done by using so-called calcium-2+-flurophores, e.g. molecules that give of light when binding to calcium ions.
These calcium markers can work on two principles, one where the calcium-binding lead to an increase in fluorescence, the output can then be measured as increased intensity.
The second principle utilizes a shift in the fluorescence spectra when binding to calcium is present.
The fluorephores has to be transported into the cell membrane, two used techniques are 

\begin{enumerate}
\item
Using an ester-group bound to the fluorephores allowing passage through the membrane.
Then when transported through the membrane, esterase will cleave the ester-group off and the flurophores are in the cytosol.
\item
The second is more mechanic and straight forward - injection is done through a very small patch pipette.
\end{enumerate}

When detecting the calcium transition (calcium activity), several instruments are to our disposal: 
The relatively straight forward and cheap Wide-field imaging that can be used with both the single and double emission (intensity and spectrum shift) techniques. 
A major drawback of this technique is that the fluorescence is recorded throughout the sample in all depths, meaning that several regions will be out of focus.
Using confocal pinholes, the set-up is called confocal imaging and the planes out of focus can be "sorted out" and only the plane on focus can be recorded.
This allows a "depth" scan throughout the specimens and thus a 3D-reconstruction can be made.

Throughout the cardiac-cells are the T-tubulus, infodlings of the cell membranes.
In cells from patients with heart failures, these tubules are disrupted, leading to a damaged flux of calcium.
This disruption can be imaged by confocal microscopy, showing distinct pattern differences in non-healthy cells compared to normal cells.
An end-goal is to use this information and bring the description of the pathology from the cellular to an organ-model, currently being studied by a mathematical model compared to a mouse-heart model.

For imaging human patients in a clinical setting, the medical professional have multiple imaging modalities to his or hers disposal.
It is therefor a very important for the clinician to decide which image modality that best suits the current situation.
The initial examination, and also the most routinely done examination done on the heart in general, is an echocardiography.
Echocardiography, as in its name, means to write (graphein) the heart (cardio) with echoes (the ultrasound-technique).
The examination can be done by any health care professional with minimal training, is very little invasive and requires nothing more than an ultrasound apparatus.
The penetration depth of the ultrasounds is fairly short, but it is enough to take a good look at the heart.
An added benefit of the ultrasound, is that when equipped with Doppler-imaging capabilities, it is possible to quantify the velocity of the blood flow.
This information is color coded for direction (e.g. one color for left-right flow and another for right-left) and can measure the speed of blood flow, giving valuable information about the blood flow.
Pathology to look for is a dilation of the heart (the heart has increased in size to overcompensate for some underlying malignancy) or hypertrophy, a "thickening" of the heart.
The last pathology can also be strictly physiological, as it is associated with chronic exercise or pregnancy.
A key parameter apart from dilation and hypertrophy is ejection fraction, a measure of the fraction of blood in the left ventricle that gets ejected in systole.

The second most used, and much more invasive technique is coronary angiography.
It is based on contrast X-ray where a catheter with a contrast agent is inserted into the coronary artery, opened, and the flow is visualized with X-ray.
The procedure is quite painful for the patient.
When the echocardiography images the heart itself, the coronary angiography images the coronary artery, the large artery that transports oxygenated blood into the heart muscles.
The pathology encountered with this procedure are often obstructions of the artery, especially patients with ischemic heart disease. 

Other image-based modalities used on the heart are PET-scans (most often with FDG) and MRI-scans, described elsewhere.

\subsection*{Session 2 - Gut and liver inflammation and animal models in cancer}

\subsubsection*{Gut and liver inflammation}

Our immune system is a complex system with numerous participants.
The picture becomes vastly more complicated when we also know that there is a close collaboration between the immune system and the microbiome.
The microbiota make out around ten to the fourteenth power of microbes.
This is a substantial fraction of the number of cells in the organism.
Add also to the complexity, one reckons there exist more than 1000 different species in the gut.
Although separated by a layer of epitelium, the immune system and the microbiota shapes each other, by exchange of molecules.
A concept introduced by the speaker is the "GxE"-concept, meaning genes multiplied with environment.
It highlights how the diversity of trait multiplies and adds up quickly, potentially a multitude of treatment related implications.
Also worth noted is that the genetics are more or less constant, but the environmental factors are highly versatile, and can be adjusted.

An example of the interplay between microbiota, immune system, genetics and the Environment is the inflammatory bowel disease, IBD.
IBD can be categorized into two main diseases, Chrohns disease and Ulcerative colitis.
Some findings indicates a strong hereditary factor in the development of IBD, but only 20 \% of the heritability is explained - meaning we are missing some part of the picture.
A single gene disorder has been identified, but the majority of IBD-cases is believed to involve a multitude of genes and environmental factors.
The environmental factor interplay has been shown by differences between populations separated by geography, and also an historical increase show that something clearly is going on.
This is further backed up with knock-out mice that when exposed to a germ free environment developed little to no inflammation, whereas in a "normal" environment they received extensive inflammation.
A wild type mouse did not show inflammation in any of the environments.

Due to the direct constant between the liver and gut, inflammation in the liver, coming from the gut has been observed.

\subsubsection*{Animal models}

The talk was about (large) animals and their use in cancer research.
An interesting initial point was that in Norway, the vast majority of research animals are fish(!) due to our extensive salmon industry.
The need for larger, more human-like organisms (not rodents, zebra-fish or flatworms) are the physiological dissimilarities between these animals and humans.
They do not have to be at the tissue, or organ-level, a lot of cellular mechanisms are different from mice to humans as well.

One curious situation presented, was a largely American phenomenon, where the use of dogs - an animal very seldom used in Norway.
The dog is quite important, it was the subject for the first (successful) heart transplant, coronary bypass, heart-lung machine and other cardio-ciculatory procedures.
And, dogs get cancer.
Owners of pet dogs have a 20-30 \& chance for their pets to develop cancer, an illness previously not treated by veterinaries.
However, due to the similarities of cancer in dogs and their counterparts in humans, a large system for pet donation to drug-trials developed, fueled by the wish from pet-owners to treat mans best friend.
Dogs then can serve the pilot, pre-clinical phase and a drug can go directly from dog to phase one human trials, or in another case, a drug can go from first in dog, or first in human, through the dog before a phase 2 human trial, optimizing treatment knowledge.

The speaker introduced his own grouping of mice experiments, 12 in total, not outlined here.
These show the broad range of the use of research mice.
Especially the genetic variety show the ever increasing library of specific mice strands, powerful tools in the field of genetics.
A striking example using the clinicians problems for deciding for the best course of action against a disease where several treatment options were available, was interesting.
Here, transplants of the patients tumor can be grafted to mice, and several therapies can be tested on the mouse tumor, eventually landing on the "best" treatment".
However, the speaker also remarked the severe limitations of using such models, when misinterpreting results from complete knock-outs, having little with the "real" clinical situation.
Further on, the tumor micro environment was mentioned just at the end of the talk, showing the need to not only have the tumor, but also the proper surroundings, to ensure a plausible clinical situation in situ.

\subsection*{Session 3 - Pancreatic beta-cells}

The talk was about a sophisticated transplant procedure helping to control diabetes.
The pancreas is an important organ in relation to numerous hormone regulation pathways, with the most known being the production of insulin.
When the pancreas no longer can regulate the insulin, or the mechanism for insulin response is failing, a disease called diabetes erupts.
This disease is a large, global problem.
In a Norwegian cohort study, one discovered that diabetes increases the mortality rate 3-5 times, and over 50 \% died due to acute and chronic complications (authors note: this should be worse in countries without proper health care).

The islets of Lagerhans are the insulin-producing cells in the pancreas.
Transplantation of these islets is an effective treatment of type 1 diabetes, with limited complications, when compared to insulin injections or full pancreas transplantation.
The transplant procedure consist of removing pancreas tissue from a donor, the tissue is hacked into small pieces and further mechanically purified, to separate the exocrone pancreatic tissue (98 \%, the tissue we do not need) from the remaining endocrine tissue.
These cells are then transfused into the recipients liver, where they reside.
The process is formalized through the so-called "Edmonton protocol" which was developed in the late 90s.
The outcome of this transplantation have increased in success throughout the years, and in a group treated between 2007 to 2010, 44 \& insulin independence was maintained 3 years after transplantation.
Major challenges for the therapy are a lack of donors and rejection of the islets due to immunologic reactions.
This challenges is difficult to address head on, as it has been shown that human and mice islets differ, initially preventing further studies of these islets in mice models.
But, a team in Oslo has made progress with a humanized mouse model, and it sounds very promising. 


\section*{Friday 18. Nov}

\subsection*{Session 1 - Biobanks}

The talk was about biobanks in general, with focus on Norwegian projects involving bio-banks.
Bio-banks is an emerging tool for the bio- and medical field.
In short, it is a large storage facility of biological samples, hopefully well categorized.
When enough meta-data is connected to the samples, they can provide the necessary amount of data to not only pure biological research, but also link environmental factors to find results concerning the vast and interconnected play between genetics and environmental factors.
This is true for both common diseases and rare genetic treats.
Fields that also have the use for large bio-banks are drug-development, and the study of bio-markers.

An important organization that brings the nordic biobanks together is Nordic Biobank Network, which includes biobanks in Sweden, Norway, Finland, Denmark, Iceland, the Farrow Islands and Estonia.
On a more national level there is the Biobank Norway, consisting of nine biobanks from Bergen to Tromsø, coordinated by NTNU.
The Norwegian biobanks (authors note: are we bragging to much?) are characterized by high quality in all levels (QA/QC/sample quality/personnel and so on).
There are several reason for biobanks to be a success in nordic countries, a research friendly population, health registries, but a key element is the personal identifier, a unique number attributed to each individual, making cross referencing between registries possible.
One of (even though there are several) the largest and most known bio-bank-related population studies are the HUNT (Helseundersøkelsen i Sør-Trøndelag) studies, with HUNT4 starting autumn 2017.
The study is vast with 220 000 screened with 126 000 of them screened with questionaire data, clinical examinations and blood sampling.
In this group, over 8000 variables are store in the databank, making it a huge and unique project.
The HUNT biobank store plasma, DNA, immortalized cells and other tissue samples in low temperature and automated storage facilities.
Some results from the HUNT2 (95-97) have been linked to the Cancer Registry to show incidence of Cancer.

When this large data material is present, it is possible to conduct research concerning precision medicine, where the genetic mark-up of the individual.
More advanced (and also alot cheaper) techniques for sequencing DNA allow us to conduct such research, which were expanded upon in other talks.
It is clear that bio-banks provide an essential part in this research.

\subsection*{Session 2 - Imaging cancer}

The talk was about imaging cancer.
Cancer imaging is a broad subject, and the link between the type of cancer (more than all the position of the tumors) and the choice of image modality is highly close knit.
Generally for all of the modalities is finding a way to make contrast between cancerous tissue and normal tissue.
First, for the superficial imaging, there is ultrasound.
It has great spatial resolution, is non-invasive, no damage associated with it and is dead simple to use.
Ultrasound do penetrate quite shallow, allowing only imaging to some limited depth.
Further more, as the speaker mentioned and the author agrees, the images are a bit ugly, and further more, it is difficult to cover the whole patient.
For smaller follow up of shallow tumors? Ultrasound is excellent.

X-rays and CT carry with them some radiation which increases the risk of long term complications.
But, it is fast (generally some seconds for the whole body scan with minimal preparation time), have high spatial resolution and are now in practically every hospital.
The CT is a work horse and is used for guiding of interventional procedures, and the straight forward X-ray can be used for lung cancer diagnostics and mammography.

The MR has excellent soft tissue differentiation capabilities.
It has no radiation, great whole body coverage with a lot of information (which can also be a drawback due to resource demand for inspection)
It is however quite expensive, and patients with large metal object can not undergo an MR-scan.

The PET and SPECT scanners are highly sensitive, inherently quantitative and can image a large number of tracers.
They have however poor spatial resolution, give no anatomic information (which is beside the point when one takes into account that they are rarely done alone but with a hybrid CT-scan) and less available.

Now, imaging is not limited to strictly anatomical information, but shifting into the realm of functional imaging.
With the ultrasound equipped with Doppler-technology one can image the speed and direction of blood flow, microbubbles in the blood stream for following flow.
For the CT, contrast enhanced perfusion can show blood perfusion.
Perfusion can also be studied by O-mediated tracers in PET, or special MR-sequences.
Perfusion can be a very interesting parameter to image, as it is highly involved in differentiating tumor volumes from normal tissue, tumors being more leaky.
With vascular imaging, one can image several parameters of the vasculature, and infer from that the micro-environment, detecting and even categorizing malignancy.
Diffusion weighted MR-sequences can allow us to do this for large regions at once, differentiating aggressive from less aggressive malignancies or monitor treatment outcomes.


\section*{Monday 21. Nov}

\subsection*{Session 1 - Personalized medicine}

\subsubsection*{Personalized medicine}

The talk was an overview of the concept personalized medicine.
Personalized medicine may be a bit of a misnomer, better words (and currently synonyms) could be stratified medicine, precision medicine or tailored medicine.
Even though it is a relatively novel field of research, the idea and concept is not new at all, as variability in patients even inside the "classical groups" (e.g. patients sharing a common disease) large individual differences can be found, and it is not far fetched to think that these differences affect the treatment outcome.
The idea got an initial large traction by the turn of the century, when the HUGO project managed to map out the total genomic profile for a single human individual.
This was a major breakthrough for genetics, and sparked an immense optimism.
However, the initial optimism was curbed by one major biological component, the expression of genes which is not uniquely identified by the genome, and a financial, sequencing the genome was immensely expensive.
But, this optimism is on its way back, as increased understanding of epi-genetics, developments in biological and hardware tools and an increasing amount of stored data will help us understand complex system we did not grasp 15 years ago.
This opens a range of possibilities for screening for treatment sensitivity, disease burden and other indicators we can mitigate with more stream-lined treatment.

However, this optimism comes with a cost.
When we are stratifying smaller and smaller populations, we loose a certain power from the evidence-based, the control group.
The gold standard for treatment development is randomized double blind trials, and with personalized medicine with one patient, one group and one disease, do not give room for a control.
This is maybe the hardest challenge to overcome, as it has to be met with a reshaping of how we look at the "best" way of developing new therapies.

\subsubsection*{Tumor Immune Evasion Mechanism}

The second part of the talk was a look at how tumors evade the immune system.
One of the "revised" hallmarks of cancer, is avoiding immune destruction.
Several talks have expanded upon the subject, but this talk gave a nice general overview.
One of the earliest indications of the tumor-immuno-relationship was described in the middle of the 1800s by Rudof Virchow that noticed leukocytes in tumor tissue.
This lead to his hypothesis that chronic inflammation and cancer were strongly linked.
Modern evidence also pointed towards this notion, as an increase in immunodeficient mice and humans with compromised immune system were observed.
A third observation is that T-cell infiltration in tumors is associated with overall survival in cancer patients.

Now, tumors has ways to evade the mechanism of the immune system.
The first step, is to avoid being seen.
This is achieved by a down-regulation of the cell surface proteins that recognize tumors as foreign objects.
A second trick is to secrete immunosupressor proteins, growth factors that inhibits the immune system to take effect.
Thirdly, the tumor over express a protein called IDO, this leads to a depletion of L-Trp in the cell microenciroment, this suppresses T-cell activity and the tumor evades the immune system.
The tumor cell can also express its own proteins on its surface that, when an effector cell is encountered, renders the attack useless.
These "co-inhibitors" are a separate way of surviving the immunological attack.
The last way of evasion is by recruiting Tregs.
The Tregs are regulatory T-cells, used to put a physiological damping on the immunoresponse.
By having a large amount of Tregs in the microenviroment, the tumor hinders the immune system.

This evasion can be remedied by a range of therapies.
Common attributes is "boosting" of the immune system by tampering the tumors evasion strategies.
Approaches is to use cytokines to  monoclonal antibodies to, to put it crudely, pump up the immune response.
A therapeutic vaccine by activating T-cells is another approach.
The last option mentioned by the speaker was checkpoint inhibitors, using proteins to block the checkpoints that regulates the T-cell activation.
This tampers directly with the tumors ability to "turn off" the attack, and have been proven as a viable strategy in lung cancer, interesting.


\subsection*{Session 2 - Drug sensitivity in leukemia}

Not present

\subsection*{Session 3 - T-cell therapies in cancer and cancer metabolism}

The idea of a cancer vaccine is not old, but for decades it proved futile to implement it in human subjects.
As described earlier in this day, the immune system have mechanisms for handling cancer.
The problem arises when the cancer evades the immune system, where they normally would have identified as foreign and eradicated.
A break-through came when we learned to active the T-cells, however in a somewhat indirect fashion, by inhibiting the inhibitory pathways induced by the cancer.
The recipe is simple, extract Tumor-infiltrating T-cells from the patient tumor, grow them outside the body and reintroduce them into the lymphodepleted patient.
This have shown good response in a multi center trial in Israel and the US.

A problem is that tumors express our own "normal" proteins, of which our immune system recognizes as "normal" and not foreign.
However, we can antibodies by vaccinating mice, as the "normal" human proteins expressed by the tumor is foreign to the mouse, thus it produces antibodies to deal with them.
However, this rarely cures patients.
We want to make the antibodies better by chimeric antibody receptor therapy.
This is done by collecting T-cells, arming them the chimeric antibody receptor which targets the cancer, and with the reinfusing it will effectively induce apoptosis in the tumor cells.
This is a very promising therapy.

Further on, this can be used to tackle on the "neo-antigens".
The neo-antigens are anti gens that lies completely outside the normal human genome.
These antigens are not affected by the local T-cell tolerance.

\subsubsection*{Cancer metabolism}

One of the hallmarks of cancer is aerobic glycolysis inhibitors.
This marks the step when tumors adjust to its low oxygen environment, and changing its metabolism.
Low oxygen stimulates the tumor surroundings to up regulate factors for increased glycolysis, and angiogenic factors.
This lead to the Warburg hypothesis, stating that, tumors glycolysis as their main way of generating energy-storage in the form of ATP.
It was believed (now refuted) that tumor cells had impaired mitochondria that affected the Krebs cycle.

However, as tumors strongly tends to get their ATP from glycolysis, attacking this synthesis seems like a nice way to treat cancer.
One way of doing this is to block citrate lyase, an important step in the cycle.
Another protein is pyruvate kinase, needed in the last step of glycolysis.
It has many varieties, each with sub-forms.
One of these forms, PKM2, more precisely the dimeric form, are overexpressed in tumor cells.
Now, the tetramer-form is more involved in APT production, but, the dimeric form is important to form nucleases and protein synthesis.
The PKM2 is also present in colo-rectal cancers, making it both a viable target for drugs and a nice bio marker, as it has been shown to be detected in stool samples.
This test has proven to be highly sensitive.
One problem however, is that it is also expressed under acute and chronic inflammatory condition.

\section*{Tuesday 22. Nov}

\subsection*{Session 1 - Medical chemistry and drug development}

The talk was about chemistry in drugs and pathways for drug development.
Developing drugs is a very costly adventure, and the drug policies for the last century has lead to large companies, supplying the world with drugs.
The boom in the industry in the eighties, with the rising of so called "block buster drugs", drugs that could be sold in large quanta, used to treat a range of common illnesses, is starting to wear off.
An emerging industry in the form of generic drugs are now present, making a "second wave" of the block buster drugs into the market.
One integrated part of the drug industry is patenting, the right to exclusively sell a drug, or a right to royalties if another company sell it.
When a drug is patented, it has to include some novelty, it is not enough to change a tiny detail in the production and call the drug something new.

Drugs are divided into classes, with their own requirements for safety and efficacy research before they are allowed on the market.
They are the synthetic (pure organic), the semi-synthetic (examples are steroids), natural (has to be harvested from nature, like morphine), biological (often very expensive), generic (a variety of another drug but with the same effect) and biosimilars (mimicking a biological effect).
Generics do not need the same strict safety testing as a completely new drug, and biosimilars are somewhat in between them in terms of requirements for safety trials.

Drugs do share some chemical similarities as almost all drugs share some common attributes.
Almost without exceptions they are organic compounds.
One is size, drugs has to small enough to pass through barriers, usually drug molecules are no larger than 500 daltons.
Further more, the drug has to be either soluble in water or fat, for absorption.
There exist a measure called the hydophilic/lipophyllic balance that defines the drugs solvent in either water (large negative values) or fat (large positive values)
The lipophyllic characteristic is especially important for the drug to pass through the blood-brain barrier.
Many drugs are used as salts as they tend be stable and highly solvable.

There are refined drugs from nature that has very complex chemical structures, some are more complex then they have to.
By this it is meant that the added complexity does not contribute to drug specificity or potency, and a synthetic analogue with a simplified structure with the same physiological effect can be made.
An example of this is morphine which can be substituted for the much less complex drug petidin.
For oral absorption there exist a very nice "rule of five", Lipinskys.
These rules of thumb state that drugs that are to be taken orally has to be small, containing no more than five hydrogen binding donors, no more than ten hydrogen binding acceptors and should not be permanently electrically charged.

The main take home message was that an understanding of chemistry is imperative to understand the drug.


\subsection*{Session 2 - Imaging disease - multiphoton imaging}

The talk was a highly interesting round-up of multi photon imaging, and have they have given insight in the world of the astrocytes.
The astrocytes are the "other" brain tissue, making up a large part of the brain comparative in the number of cells to the neurons.
They were discovered and described by Cajal in the 1900s, however, since they did not respond to electrical stimuli, it was difficult to study them directly.
More recent discoveries with sophisticated electron microscopy revealed that they contained an overwhelmingly number of aquaporins, AQO4.
These porins are proteins that facilitate rapid and selectively, not passively, transport of water molecules.

Around the glial membrane cells, adjacent to vessels, nearly 50 \% of the area is containing these proteins.
Interestingly, these porins seems to worsen the damage of brain edemas and stroke, begging the question why they are present in our brain?
One tool to investigate this is the two-photon laser scanner.
With it, fluorescence can be detected with micrometer precision and with a temporal resolution of a few milliseconds.

Extra attractive is the fact that it can be done on live mice brains, ether with a temporary coverglass with a part of the cranium removed, directly over a thinned cranium, or as a chronic window, where a cover-gass is attached chronicly to an opening in the skull.
Through fluorophores, encountered and described in relationship with the calcium imaging, the flow of fluid in the brain can be monitored in practically real time.
With this technique, it was shown that there exist a "flushing" of cerebral spinal fluid through the brain, facilitated by the aquaporins.
This is believed to work in analogue to the lymphatic system, as the brain lack lymph nodes and need a process to get rid of toxic waste products.
The route of this transport is debated, on one side there could be a convective flow, pushing the waste into vessels.
Another theory relies on the capillary network, as the distance traversed are more in line with the time frame.

The second application of the two-photon microscope introduced, was tracking a depressing wave through the brain.
This depressing wave, where the mystery was what mediators was in charge of this wave.
With the two-photon microscope, it was possible to spatially and temporally track the mediators, and a slight time-shift was found between them, shedding light on the process.

\subsection*{Session 3 - Predictive medicine, epidemiology}

The talk was about predictive medicine using large registers for epidemiology information, examples were several markers vitamin deficiency and cancer.
The Nordic countries, as note before, have a large advantage when compared to many other countries, the population has a unique identifying number.
This makes cross referencing several registers easy.
We have also collected data on cancer for some time, an initiative in Denmark was started in 1943, lead by the same individual until 1980, however, the oldest was established in Hamburg (1926).
Main variables of interest are sex, time of diagnosis, date of birth and the cancer characteristics.
The cancer site can be further classified into standardized location, allowing further cross examination.
As cancer is a variable disease, the histological type of cancer is also of interest.
Further down in interesting parameters are time and basis of diagnosis.
A last, non biological parameter is the place of residence, this is interesting as life style and environments follow geography, and can help to pin-point otherwise complicated parameters.

The work-flow include three main institutions, clinical research, basic research and epidemiological research.
This work-flow is by no means linear, a discovery can originate in either three, and be explored and further investigated in the other two.
The D-vitamin deficiency example presented show a nice interplay between these research fields.
A lab experiment showed that vitamin D bound to receptors that inhibited cell growth, angiogenesis and promoted cell differentation, apoptosis and immunnologic mechanisms.
This should imply that without vitamin D, cancer should increase(!)
In Norway we have a strong UV-gradient, but NOT a geographical variation in cancer mortality or vitamin D-levels (due to fish consume in the north?).
However, the Janus serum databank showed that low vitamin D-levels worsened survival.

The other examples showed the same interplay between epidemiology, the clinic and the lab.
Somethings are a bit fuzzy with a lot of life style factors, as these are highly volatile and different to measure (large amount of subjectivity).

\section*{Wednesday 23. Nov}
\subsection*{Session 1 - Venous thrombosis}

The presentation was about venous thrombosis, the disease, the pathology and the risk factor associated with it.
Venous thrombosis happens quite often in the population, with 1-2 incidents per 1000 people.
Mortality is associated with a 30 day case-fatality rate of 5-10 \%.
The common type is the deep vein thrombosis(DVT, 2/3 of the cases) which originates in the vessels of the extremities.
The more serious, Pulmonary embolism (PE) accounts for the remaining third of cases.
It is assumed that the incidence of VTE will increase over the years, leading interest in identification in risk factors and potentially mitigate the problem through treatment.

There are believed to be three major conditions that has to be present for VTE to develop.
A vessel wall injury, which can stem from degradation due to aging, surgery similar damage.
Stasis, caused by obesity, pregnancy, still-life due to surgery or general immobility.
The last one is hypercogulability, which can come from obesity, pregnancy, hormone disturbance or thrombophilia.
On the more microscopic scene, a leading hypothesis suggests that Monocytes and other immunocells get trapped in vortexes cause by the flow through the valvular sinuses.
These vortexes also cause an intermedittant hypoxia.
The monocytes adhere to the endothelium and causes build-up of fibrin.

We know some things of risk factors in VTE.
A first and striking point is that the risk increases dramatically if the patient is hospitalized.
There are also a larger incidence rate in age, with the increase starting at 30-40 years old.
As obesity is a condition related to both stasis and hypercogulability, it should be an indicator for VTE as well.
However, it is somewhat complicated as the exact mechanism is unknown - it could either be pressure on the body from excess mass, or a more linked interaction between metabolic dysfunction and molecular pathways.
Some genetic risk factors has been identified, but the whole picture is not known as there exist a gap between observations done in familiy and twin studies and genetic risk factors.
This urges us to investigate the underlying genetic risk factors
A hypothesis presented was a "potential model" where several factors were needed to push the risk up and above a level which induced VTE.

VTE risk factors can be found together with risk factors for connected diseases.
Especially cardiovascular diseases are interesting as thee diseases share some risk fators.
There has been found a correlation between VTE and myocardial infarction, especially in the time after the infarction.
The same association can be found with stroke, however only in a short period after the stroke.
It remains an unanswered question if there are strictly common risk factors leading to both VTE and arterial thrombosis, or if risk factors lead to arterial thrombosis which in turn lead to venous thrombosis.
The two other co-morbidities mentioned were cancer and hospitalization.
For cancer, an increased risk was found both after and before the incidence of cancer.
As both before and after cancer increases risk, the mechanism is not limited to the treatment, however, treatment of cancer is a intuitive risk factor.
The connection between cancer and VTE have lead to simple prediction scheme, where a score for the risk of VTE is given an the basis of cancer site, blood values and BMI (the Khorana Risk Score).
A similar risk score is found for hospitalization, with the basis of previous complications.

\subsection*{Session 2 - DNA repair and epigenetic}

\subsubsection*{DNA damage and repair}

The talk was about DNA-damage and repair, describing the different ways DNA can be damaged, repaired, and the consequences it has for the genomic integrity.
The DNA is not a specially stable molecule.
It is constantly under "attack" from radiation, erroneous replication, secondary damage from radicals and other dangers.
There are however, numerous repair mechanisms for when things go wrong.
The way DNA is repaired is strongly linked to the type of damage.
The vast majority are structurally damages to the DNA-helix through chemical alterations of the bases, inducing bulky lesions from non-native chemical bonds.
The DNA can be repaired "directly" through photolyses, alkyltransferase or ligase processes.
In more severe cases, nucleotide excision repair, recombinational repair or mis-match repair is needed.
If the cell fails to repair, if will either be placed programmed for cell death if it recognized.
If the fail to repair is not recognized, it can lead to a cancer.

\subsubsection*{Epigenetic}

Epigenetics deals with differences in transcripted DNA where the DNA itself do not change.
They are categorized in entities called epigenetic "marks"
DNA mutations are permanent compared to epigenetic marks, as these marks can not only be turned off, but completely erased.
The changes are happening naturally on a "higher" level than the DNA.
First stop is the chromatin, where it has been shown that there exist two kinds, one that is transcriptonally active, and one that is silent.
It also looks like active and inactive (gene rich and gene poor) parts of the genome are organized in "fixed" positions inside nucleus.

Now, an important concept are topologically associating domains (TADs).
These are clusters of the genome which can have many or few interactions, and their structure is conserved across tissue types.
Diseases are associated with disruption of the end-regions.
As the chromatin is arranged in loops, these loops can bring parts of the genome together, effecting transcription by imposing a direction, promoting or silencing transcription.
This is done by structural manipulation.

Another level of manipulation are using the nucleosome to postion gene locus "on" and "off".
This manipulation is done through exchange of histones, swapping, deleting and reorganizing them with the help of special proteins.
When this regulation of the histones go wrong, diseases can arise.

DNA-methylation, inactivating of the genes through the bounding of methyl groups on certain bases can effectively silence genes.
This could potentially be a valuable indicator of cancer, as cancer cells tends to have highly methylated DNA compared to their normal tissue counterpart.

\subsection*{Session 3 - Cardiovascular disease}

\subsubsection*{In animals and humans}

The talk was an introduction to the use of animal models in heart-diseases, first a general introduction and some examples with mice.
Heart failure affects a large part of the earths population, with 2-3 \% of the population affected.
Ischemia, developed from atherosclerosis is a major cause of death in the first world.
Clinical outcome of heart failure has came with the use various drugs, but it remains a "big killer".
This is largely due to somewhat poor understanding of the underlying mechanisms.

Research on human has its limitation, thus we need good animal models to emulate the human organism.
A lot of prior success-histories in heart treatment has come from the dog, pig and rodents like rats, mice and rabbits.
Of course, animal models comes with some caveats, no animal model is perfect and differences from human physiology arise quickly.
Apart from biological challenges, one faces also legal, ethic and cost-challenges, all of which has to be considered.
A little conundrum is also that the more human-like our research subjects become, the more ethical problems arises.

Due to the similar heart physiology, pathophysiology, genetic make-up and short life-span, the mouse and rat are favorable animals for studying the heart.
These rodents can be used as test subjects for surgical intervention, mimicking heart operations or induce acute heart failurees by blocking vessels.
Intervention in the form of pharmacological remedies to emulate heart failures is another approach, or introducing infection agents to trigger autoimmune reactions.
Putting mice or rats on a strict diet can also help to understand how life-styles including high salt, sugar or fat can affect the heart.
A still growing library of knock-out mice also serve as helpers to understand the heart.
It allow us also to study "clean" models where we can have genetically obese and diabetic individuals without hypertension.

Other animals as non-human primates, the zebra fish and the fruit fly are also of course used for medical research.
The primate, although seldom used do to cost, handling and ethical concerns, has helped our understanding of drug dynamics and heart failure related to HIV.
The zebra-fish and fruit fly, as expanded elsewhere in this report, gives good genetic tools.

Hearts with heart failures have generally undergone some kind of remodeling, either a dilation or hypertrophic changes.
These restructuring have been linked to the extracellular matrix, surrounding the heart cells.
Some interesting proteins, the proteoglycans, have been found to be related to hypertrophy, one study found Syndecan-4 knock outs to not develop concentric hypertrophy.
This could indicate that these proteins can be used as potential bio-markers.
Some interesting bio-markers found 

\section*{Thursday 24. Nov}

\subsection*{Session 1 - Structural biology}

The talk was about structural protein discovery, exemplified through work on APT-ase
X-ray crystallography is an imaging technique that can, quite briefly said, be used to image molecular structures.
When it was discovered in the 30s that proteins, when purified, also did diffract X-rays in a well behaved manner, it opened up the possibility to image the structure of proteins.
Proteins are large structure, containing a multitude of atoms, and further numerous sub-parts making up a complex geometry.
This complex geometry is necessary for the protein-function, and therefore it is crucial, and highly interesting, to image the protein structure.
Due to the geometric complexity, the first protein was not fully described before 1960.
Now, with the advent of computationally  powerful hardware, it is possible to study more and larger proteins.
Of course, there are many other difficulties than the mathematically calculation of the structure from the diffraction pattern.
Before the X-ray itself, the protein has to be biologically identified, produced in a relatively large amount.
It then has to be purified, crystallized and be ensured to maintained its crystallized structure.

After the crystallization, the imaging itself is highly technologically difficult, using large facilities that produce highly collimated, single wavelength X-rays.
The crystals has to be rotated to produce the diffraction pattern.
The diffraction pattern are then used to calculate and visualize the electron density, this could be considered the "raw data" in crystallography.
Now, a model calculation is performed to transfer this electron density into a final structure.
In this step, there is a clear distinction from the previous "image and calculate" to "calculate and interpret" by using different chemical and mathematical model to work out the underlying structure.

\subsection*{Session 2 - Colorectal cancer}

The talk gave insight in Colorectal cancer, and why it is a favorable disease for the development of bio-markers for.
Colorectal cancer is a disease that affects a large number of patients, and have a high mortality.
There are some geographical spread, and it looks to affect mainly people in the first world, with a steadily increasing incidence rates in the Nordic countries.
It is also expensive to manage, relying on expensive procedures, and the mortality increases sharply with illness progression.
If colorectal cancer is discovered early however, it is highly manageable and this "window of opportunity" from illness to fatal progression is quite wide, spanning years and sometimes decades.
All these factors make colorectal cancer a suitable candidate to develop screening programs for.
The "gold standard" for colorectal cancer screening is endoscopy, either in the form of a colonoscopy (the whole colon) or a sigmoidoscopy (only the lower part of the colon).
There is a large screening project currently being explored involving endoscopy, however, the procedure is quite costly and invasive.
A less invasive procedure is the stool sample, the Fecal occult blood test, being performed in a selection of European countries.

The attention is then directed to viable biomarkers, which can give an indication for malignancy in the patient sample.
The marker has to have a high degree of specificity, meaning it cant be present in normal tissue, and sensitivity, present in the majority of tumor tissue.
A group of biomarkers presented is methylation of certain parts of the DNA.
The methylation of DNA in cancers are highly abundant, more frequent than mutations, and can be identified early in the disease development.
This makes the monitoring of methylation a viable and interesting biomarker.
As of now, six distinct biomarkers has been identified, and methodologies to detect them has been developed.
All six performs well, with a panel having a sensitivity of adenoma and carcinoma of 93 and 94 \% respectively, with a 98 \% specificity compared to normal mucosa.
This means that although the test will give some positives in the form of adenomas, it will detect the majority of carcinomas with very few false positives.
This is very interesting as it utilizes epigenics for screening, with a high degree of accuracy in a troublesome disease.

\subsection*{Session 3}

Not present

\section*{Friday 18. Nov}

\subsection*{Session 1 - Beta-cells and diabetes}

Diabetes type 1 and type 2 are complex diseases depending on a lot of factors - chronic conditions, lifestyle, in utero development, parental obesity (recent in nature), environmental factors, nutrient, intertestional flora, aging (especially type 2)
Common for them is a lack of beta-cells, residing in pancreatic islets.
The first step to cure this disease is then to find the reason why the beta-cells are dying, and further, have can we stop this from happening?

A challenge is the fact that he humans are bad research subjects lacking the possibility of ethically unproblematic genetic manipulation, highly variable background and variable life-style.
Use the previous introduced  iPSC, take different conscription factors and end up with a mature $\beta$-cell (a multi-step process)
Possible to take the last steps (the last steps) into a mouse model.
Purity is a problem, along with: it is not systemic, it is inefficient, inaccurate and the output is very premature, we obviously need more tools(!)
Of course, the situation is complex, and no perfect tool is present.

Different murine systems:

\begin{enumerate}
    \item
	Ob/Ob mice - mutation of leptin, they gain weight rapidly and develop hyperglycemia, this is a T2D-model.
   \item
	Db/Db mice - mutation of leptin receptor, more severe symptoms than Ob/Ob, this is also a T2D-model
    \item
	Akita mice - mutation of insulin2 - note obese, selective toxicity depletion of beta-cells. Develops insulin dependent diabetes - resembles very much a T1D-case and is therefor used 
\end{enumerate}

Chemically induced abation - streptozotocine and alloxan - inject and wait for beta-cells to be depleted.
Alot of problems with this approach, it is toxic, unspecific, inefficient and inaccurate.

Transgenic models, harder and more expensive.
Have a genome, add a transgene, specific or random. 
A better approach is to use a specific promoter that trigger transcription and inject into a host-genome

Transgenic and toxin induced ablation - Diptheria toxin - and a DTA-system where you kill 75 \% of $\beta$-cells.
Downsides: Patial and variable ablation efficiency and requires inducible systems.
Use a promoter and mark a cell for killing - kill exceptionally specific and efficent (99\% $\beta$-cell loss).
The downside of the promotor-method is that you need to reate a transfenic line for each promoter.

The third model: Transgenic and specific gene targeting, the most refined and nice model.

Need to trace the cells, remove the stop-codon before the gene for a green fluorescence.
Problem: Cells are irreversible traced from the moment they express the promoter.
This means we can have two identical cells of unknown origin.
We can use conditional cell-tracing to separate them from eachother.
This allow us to identify apparently identical processes, it is highly specific.
We can observe the outcome of the cell and some observations has been done:
First, cell death is not the only decay mechanism, decreased proliferation is a factor two.
Two additional factors are differentiation related, they de- and trans-differentiate.

Now, how can we regenerate the beta cells?
Some observations has been done in mice:
We know that sadly, some observations that sparked interest in regenerative inducing in beta-cells observed in mice are not transferable to humans.
The reason for this is that human beta-cells grow very slowly.
Naturally some beta-cells are needed if a proliferation-based regeneration is wanted.
However, total absence of beta-cells will induce some cell-interconversion, meaning formation of beta-cells will start.
This phenomena has been observed in humans(!)

%We can use cycline to kill the cells.
%Beta-cells not really dying but stop to produce insulin (real reason for diabetes)
%
%(lots of stuff)
%
%Things to remember:
%
%The 2 most prevaltnt types of Diabetes are complex disorrders and hard to charectirize (esp. in humans)
%
%There are many models (in vitro, spontaneus, chemical and transgenic)
%
%Cell death is not the only mechanism of beta-cell decay, dedifferation, preliferation, transdifferation etc.
%
%Genetic cell tracing esp. inducible systems are required to properly analyse cell fate
%
%\emph{Regenerative strategies} - proliferation, trans and redifferantion and neogenesis
%Different tissues can do one or several, or all.
%
%The innate and spontaneus B-cell regenerative
%Loss of 50-70 \% self-renoval by cell division, in extreme cases (99\% loss, conversion)
%
%The best is cell-division - as it is fast.
%Obecity and pregancy show increase in beta-cell mass.
%Beta-cell is able to increase proliferation upon stress and loss (due to for example injury).
%Chemical drugs can also induce proliferation, Glucose, CLP1 and S961 (this is specific and a dose dependent effect have been shown, human cells do not react to this drug).
%
%The second scenario is when you have almost no beta-cells left.
%The pancreatic islet have different cells, alpha, beta, PP cells and delta-cells.
%RIP-DTR ablation system to model T1D, killing 99\% of the beta-cells, leading immidiantly to diabetes. 
%The mice can regerate up to 70\% of the normal amount in adults.
%Use alpha-tracking show that the beta-like cells started as alpha-cells. 
%They stopped to express glucagon and started expressing insulin (hurray!)
%Not in humans (?)
%
%Cell interconversion also occurs after partial loss.
%Interconversion can appear in humans (in T2D not T1D)
%
%Summary: The dynamic of b-cell decay failure, alter the choice of the 
%
%\begin{enumerate}
%    \item
%	Human B-cell proliferate slower
%    \item
%	A minimum number is required for preliferation based strategies
%    \item
%	Some B-cell proliferate drugs were promising in mice 
%    \item
%	Total absence will trigger other mechanisms
%    \item
%\end{enumerate}


\subsection*{Session 2 - At the interface of chemical kinetics and population biology}

This session was about system biology, using mathematical modelling in infectious and cancer diseases.

\subsubsection*{Infectious diseases}

Post-atibiotic era: No new antibiotics (no new classes, classes have a common target and similar chemical structure).
These classes often have similar resistance mechanisms - resistance is a global problem that is being acknowledged.
Talk of two diseases, Tuberculosis (global problem) and hospital acquired infections.
Large drop in TB the last 100 years in Europe.
This is not to be seen in resource-poor countries. 
Strongly resembles HIV-prevalence, the reason for this is that 5-10 \% is infected but not sick, but with HIV you develop disease and starts to spread disease.
Tuberculosis is a very underestimated disease - underestimated by 2-folds by the WHO according to the speaker.
Biggest problem is multi-drug resistant tuberculosis - worst in eastern Europe. 

Death toll in Europe for HA-infections os much larger than multi-drug-resistance and HIV-infections (comparable to breast cancer).
8 \% of hospital stays result in infection, of which half are multi-resistant.
Treatment right away is very important.

Erlich formulated a simple saying some years ago: Hit hard and early, little to no evidence for this but we still do it.
Important questions: 

\begin{enumerate}
    \item
	How often?
    \item
	How much?
    \item
	How long?
\end{enumerate}

There is a problem however, as the different levels of interaction are hard to connect.
This can be done by a really simple but robust and plausible mathematical formulation.

Set up an experiment to monitor replication in a single cell.
There is some kind of "lag", even after the drug has stopped being present, the bacterial wait a bit to start replicate again)
This can be explained by the existence of a reservoir of unspecifically bound antibiotics.
Antibiotics need to "leave" the cell.
Explosive onset of growth could mean that antibiotics "diluted" by replication.
This can be used to explain how often one should give antibiotics.

The end-goal of this work is to stream-line drug-development.
This can be done by mathematically formulating the best time to start and stop a therapy with antibiotics, using the simple kinetic models.

% Cancer starts here

\subsubsection*{Cancer}

The talk was about how to use a system approach to cancer the genome.
A broad description of the approach was first described then a presentation of the speakers own work.
The last part was a bit fuzzy for the author.

Systemics in molecular biology tackles the problems that arises by assuming that a large and complex system can be understood through an understanding of the parts of the systems.
This is the basis for all systemic approaches.
Specifically for molecular biology, the speaker proclaims that a biological system is more than the sum of its parts.
This means that it is the interactions between the different parts are as important as the parts themselves.
A network approach tries to connect the genes, proteins and chemical reactions into a large network.
This is done by gathering all available data, be it literature, gene expression, chemistry and so on, connect and integrate them together and see if good predictors can arise.
If done correctly, this can simulate highly complex systems in a self-adjusting, nice and well behaved manner, if done incorrectly, it can make a large mess.
The self regulatory part comes from constant updates with experimental data, adjusting if the system does not fit with reality.

Cancer can be though of in this context as a large system suffering from a perturbation.
The perturbation starts in the genes, some mutation or aberration in the gene material, and the effect is gene expression and malignancy.
In between is a large network connecting the cause (gene mutations) and effect (gene expression) and a nice model for predicting a certain outcome from a certain set of observations can be stated.

The author lacks the vocabulary to discuss their findings, but it looks interesting.






%Combine the whole gene, protein and chemistry parts into a whole system.
%Two typical work-flow is to gather data, integrate and make into a correlative model so we can predict outcomes.
%
%Cancer evolves when the system get perturbed, as cancer is an error on the genetic level.
%
%Some causes, and effect - this can be translated to an interaction network.
%
%Now possible to sequence whole genom quite cheap.
%
%Also look at regulatory gene-coding, not just protein-coding.
%
%Transcriptions and proteomes are the key to underestand how disruption of this can cause cancer.

\end{document}

