\documentclass[12p]{article}       
\usepackage[utf8]{inputenc}
\usepackage{parskip}
\usepackage{graphicx}       				% For å inkludere figurer.
\usepackage{amsmath,amssymb} 				% Ekstra matematikkfunksjoner.
\usepackage{siunitx}% Må inkluderes for blant annet å få tilgang til kommandoen \SI (korrekte måltall med enheter)
\usepackage{float}
\usepackage{geometry}


\def\ps@pprintTitle{%
\let\@oddhead\@empty
\let\@evenhead\@empty
\def\@oddfoot{}%
\let\@evenfoot\@oddfoot}

%	\sisetup{exponent-product = \cdot}      	% Prikk som multiplikasjonstegn (i steden for kryss).
% 	\sisetup{output-decimal-marker  =  {,}} 	% Komma som desimalskilletegn (i steden for punktum).
 %	\sisetup{separate-uncertainty = true}   	% Pluss-minus-form på usikkerhet (i steden for parentes). 
\usepackage{booktabs}                     		% For å få tilgang til finere linjer (til bruk i tabeller og slikt).
\usepackage[font=small,labelfont=bf]{caption}		% For justering av figurtekst og tabelltekst.

% Disse kommandoene kan gjøre det enklere for LaTeX å plassere figurer og tabeller der du ønsker.
\setcounter{totalnumber}{5}
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.35}
\geometry{margin = 0.75in}

\begin{document}

\section*{Monday 14. Nov}

\subsection*{Session 1 - Introduction to animal models and gene editing}
The main reason to use animal models is to exploit the similarities between humans and other organisms, thus preventing the use of human test subjects.
The similarities is maybe most present when embryos from different animals are compared, even though the adult individual does not look the same at all, the embryoes are nearly identical.
It is a major step up from cell-like studies, as a whole organism can be used as a model.
Animal welfare is very important, the three R's (replace, refine and reduce) is a nice rule of thumb.
The animals range a wide variety of complexity, from the simple flatworm (Caenorhabidtis Elegans) to the more human-like rodent, the rat.

The most "classic" animal, the rat, is a relatively human-like organism.
They have a generally short life-span, speeding up experiments.
However, the life span of rats in an experimental setting is considered long, when compared to flat-worm or fruit-flies.
Recently, apparently less human-like animals have been used, as the flat-worm and zebra-fish.
These animals do not look like humans, but we share a great deal of cellular and molecular path-ways.
This, combined with small size and short life-spans make them excellent research animals in molecular and genetic studies.

Gene editing has become a particullary powerfull tool when investigating diseases.
The work-flow can for example start with a discovered gene attributed to a human disease (say an epidemologist recognize a particular trait in closely related individuals).
The task at hand then becomes to implement the discovered gene in an animal model, either by knocking out the particular gene, overexpress it or a combination of these.
It can also go the other way around, when scientists discover a mutation in a particular animal model that lead to a disease in the animal that resembles a human illness.

Roughly, three types of genetic animal models exist:

\begin{itemize}
\item
    Knock-out:These are the more extreme cases. 
    The gene that is studied in the animal model is completely knocked out.
\item
    Transgenic: Here, a normal gene is substituted with a mutated variety of said gene.
    This could be considered less extreme as it doesnt neccecary involve development of the animal.
\item
    Conditional knock-out: Highly specific knock-out allow us to delete a gene in a specific tissue.
    This allow us to develop highly tailored animal models.
\end{itemize}

The tool that allow us to develop these animal models have evolved during the past years, with the introduction of the CRISPR/CAS9 technique as the main contributor.
The technique is encountered everywhere in genetic research, and put short, it allow us to introduce highly specific DNA-double strand breaks.
These breaks further allow to substitute, or completely cut out a gene expression with a high degree of specificity.

\subsection*{Session 2 - High output screening and the zebra fish}

\subsubsection*{High output screening}

High output screening (HTS) is a highly advanced, highly useful technology. 
In short, it is the classical biochemical test, combine compounds in a well and observe the outcome, on an industrial scale.
This is achieved with robotics and a high degree of automatication.
The HTS system consist of multiple parts, main system parts being the assay plate, robotic handling, compound storage, detection and data integrity.
The assay plate should be standardized and fit to the other compartments; a much used format is the 384 well plate.
The robot handling is an industrial robot that can transport and store the assay plates. 
A critical part of the handling process is the liquid handling - now highly reffined techniques can use mechanical (sound) waves to dispence volumes down to 2.5 nl.
The various compounds is stored in "libraries" that can stretch from a couple of hundreds to several millions of different compounds.
This storage aspect means that a great deal of effort is used towards storing, to avoid degredation.

\subsubsection*{The Zebra-fish}

The zebra-fish was mentioned in the chapter of animal models, but was more thourough presented in its own session, and that it highly deserves.
The zebra-fish is a vertebrate with highly genetic and physiological similarities with humans.
It developes very rapidly, with a gestaltion time of XX weeks.
Here also lies one of the greatest attributes of the zebra-fish - the development is ex-utero, with a transparent infant stage.
This means we can follow the development of the individual in a very direct and nice manner, making it an excellent model for studying fetal development.
Due to its small size, relatively small amount of compounds are needed, and the zebra-fish absorb it directly through skin and gills.
This means we can place the fish (either adults or more likely, embyonic) in an enviroment containing the compund, and thats all that is needed.

\subsection*{Session 3 - Pharmacology and the G-protein coupled receptors}

Pharmacology is a large field that tries to answer a set of questions, mainly: How can a drug be \emph{effective} and \emph{selective}?
For the drug to be selective, it is meant that it acts on the desired target, and preferably that target alone.
The optimum scenario is naturally that the drug only works on the selected target, but this ideal is seldom met, and the reality is a trade-off between the optimal therapeuic effect and the minimal side-effects
Drug targets are classified into various classes, large ones being receptors, ion-channles, transporters and enzymes. 
These classes sometimes overlap.
The receptors have their own subclasses, common for of all them is that the primary task is to mediate some kind of signal.
This signal is passed on by the receptor recognizing some transmitter (a ligand) and undergo some chemical change that has an effect.
A large group of receptors are the G-protein receptors.
The G-protein receptors contains several sub-units, allowing complex trans-membrane signalling.
Classification of all the types of G-protein receptors is a field of its own, they can be classified by ligand, which effector system they utilize; or maybe a genetic classification is the best?
Drugs can work as both antagonist (blockers) and agonist (imitating the ligand).
Drugs that are associated with the G-protein receptors are various adreenergetics, chelinenergics and serotonin and histamine analogs.

The catalytic transmembrane proteins spans the membrane and the classical example is the receptor tyrosine kinases.
The ligand lead to a phosphorylation, changing the structure of the protein on the inside of the cell membrane, and mediates the signal.
Ligand gated ion channels goes into the realm of ion channels, but a chemical ligand is needed to activate (or de active) it.
Some steroid hormones work on these ligand-gated ion channels, vitamin D, vitamin A and several thyroid hormones.

The other large receptor group are the ion channels, serving as a gate-keeper for ionic potential  across the cell membrane.
These could be voltage gated (relying on an incoming depolarization) or mechanical-working, relying on a complex system of proteins.
Ion-channels ventures into the realm of transporters, which facilitate transport across the cell membrane, either biomolecules, ions or water.
A classic transporter is the synaptic nerve system, and inhibitors of these transporters are the basis for many antidepressants.
Enzymes can also acts as drug targets, changing the structures and render them non functioning.

When looking at dose-effect relationship, one can encounter a quite straight forward mathematical mode, where the effect of the drug goes as a sigmoid of the dose (reaching a plateau where the effect is saturated).
The half-way point is often a meassure for dose-rate effect, the dose to reach half the saturated effect.
However, more complicated situations can arise when a mix of agonist and neutral antagonists are present, or, if the ligand acts via multiple receptors, making the picture quite complicated.

\section*{Tuesday 15. Nov}

\subsection*{Session 1 - Inflammation and pluripotent stem-cells}

Inflammation. 
Interestingly, inflammation is not neccecary an infection, the infection is the \emph{cause} of the inflamtion.
It is one of the bodies most quick and efficient protective response.
The main "job" of the inflammation process can be summarized quite nicely in the "five Rs of inflammation":

\begin{enumerate}
    \item
	Recognition 
    \item
	Recruitment 
    \item
	Removal
    \item
	Regulation
    \item
	Resolution
\end{enumerate}

The first step is recognition, a foreign object must be identified.
The situation with the foreign object is the most "classical" scenario when we think inflammation, but it does not have to be an element from "outside".
Damaged tissue can also initiate an inflammation process, either caused by trauma or ischemia.
The foreign object does not need to be a bacteria or virus, it could also be silica crystals or other non-biotic elements.
Inflammation can also be triggered by an immune reaction due to allergies or body transplants.

When the inflammation is initiated, some macroscopic symptons can be identified.
These symptoms has been described since ancient times, and includes heat, redness, swelling, pain and less of function.
The pain is maybe the less "intuitive" symptom, as it is little to do when the infection occures, however, it helps to "immobilize" so the body gets much needed rest.
The microscopic scene is fairly complex and involves several cellular participants, but the main early mechanism is to recruit cells to deal with the damage. 

\subsubsection{Pluripotent stem-cells}

The talk was about stem-cells, introducing some concepts, posibilities, nomenclature and examples.
There are two kinds of stem cells, embryonic stem cells and adult stem cells.
The embryonic kind can divide indefinitely and can differentiate to every type of cell.
Adult stem cells are more limited, they can regenerate certain kinds of tissue and are much more difficult to culture in vitro.

The potential of stem cells can be divided into totipotent (the zygote that can give rise to an embryo), the pluripotent and the multipotent.
All organisms generally start as a totipotent cell, and further and further differentation limits the specific cell potential, down to a unipotent cell.
This can be though of as a process where genes are sequentially turned off, it is then naturally a riveting though to how these cells can be "turned on" again and get back to their "high potential state".
A set of transcription regulators have been identified with this process, Oct4, Sox2 and Nanog.
To maintain pluripotency, Oct4 and Sox2 are needed, and Nano acts as a safeguard for the pluripotency.

Embryonic stem cells are the ones with the greatest potential, they can be, if held in their proper state, used to produce all the different cells of the body.
The need for fertilized eggs do naturally bring on some ethical concerns.
It is therefore tempting to explore the possibilities to take a differentiated cell, grow it together with the aforementioned pluripotent state proteins, and get induced pluripotent stem cells, which in turn can be differentiated into the desired tissue cells.

As we then can harvest cells from a donor, and use them to make tissue- and patient specific, a powerful tool in personalized medicine is acquired.
Attempts are promising but are riddled with challenges, the biggest being sub-par and unoptimized differentation protocols and circumvent problems induced by epi-genics when recreating an adequate cell culture.
However, if we overcome these obstacles we can grow organs meeting the transplant need with patient specific tissue, test for patient specific toxicity and numerous other application when patient specific tissue samples are needed in larger quanta.

\subsection*{Session 2 - Molecular pathgogenesis and tumor angiogenesis}

The talk was about the hallmarks of cancer, with focus on angiogenesis.
Cancer is a large disease in terms of affected number of individuals.
The incidence has risen steadily for both male and female patients, except for stomach cancer.
The increase in incidence comes to some extent from life-style changes, but also from screening and earlier detection of malignancies that some years ago would go on undetected.
A neoplasm, or tumor, is an uncontrolled growth of tissue that invades and spreads to other tissue.
In stained tissue samples they tend to look star-shaped and more irregular than their non-tumorous counterparts.
The tumor has a characteristic micro environment, with different cells making up a fairly complex environment.

The characteristics of cancer, or apply called the "hallmarks of cancer" was coined and published in a seminal work by Hanahan and Weinberg in 19XX.
They there described six hallmarks.
This was revised in 2011 to include additionally four, ending in a total of ten hallmarks.
These hallmarks are here described:

\begin{enumerate}
  \item
    Sustaining proliferative signaling:
    The body has mechanisms to avoid proliferation without limits.
    These mechanisms are among other factors, a series of molecular signals, hindering the cell to divide.
    When this hormonal system is tampered by maintaining a constant "positive" signalling for proliferation, neoplasms can occur.
  \item
    Evading tumor growth suppressors:
    It not important just to maintain a positive growth, but to hinder "negative growth signals" in the form of regulator proteins as p16-RB, p21 and p53.
    Specifically, as shown by a study from 2002, patients with melanomas that do not express p16 have a much worse survival out-come.
  \item
    Activating invasion and metastasis:
    The tumor tissue, in order to spread, must have an ability to break through the epithelium and wander around in the organism.
    This is done partly by the production of proteases that degrade the basal membrane, or through so called epithelial-mesenchymal transition(EMT).
    EMT is a process, starting with the loss of E-Cadherin, that in turn makes the basal membrane loosing its cell to cell adhesion capabilities, allowing cancer cells to move through and into the blood stream.
  \item
    Enabling replicative immortality
    Tumor cells have a unique capability to sustain "indefinitely" given enough nutrients.
    This is achieved by tampering with the check-point mechanisms that ultimately kills cells when they have reached a certain number of divisions, the telomers.
  \item
    Inducing angiogenesis:
    Uncontrolled growth without a proper blood-supply is difficult.
    Therefore, the tumors has to induce growth of the blood-supply as well.
    The previous there was the tumors got some infiltration of blood vessels, and then expanded the size of these vessels.
    A new understanding is that the blood vessels in dead reach further into the tumor as the tumor is expressing angiogenic promotion proteins.
    These proteins are in normal functioning tissue associated with wound healing, and thus the cancer can be though of as a wound that never heals, taking advantage of this situation.
    The proteins are various and numerous, but the most classical ones are the "positive" VEGF, bFGF, but controller do also exist and are associated with angiogenesis.
    A third factor is bone marrow involvement, where haemapoetic stem cells are recruited to differentiate into endothelial stem cells.
  \item
    Resisting cell death:
    Tumors are foreign tissue and should under normal circumstances be cleared out by ether apoptosis, necrosis or autophagy, preferably in a controlled fashion (apoptosis and autophagy)
  \item
    Deregulating cellular energetics:
    It is believed that tumors have different metabolism than normal tissue, the most extreme being a metabolism that exclusively relies on glycolysis (The Warburg-effect)
  \item
    Avoiding immune destruction:
    The immune system have ways of destroying cancer tissue if they can be recognized as foreign.
    The tumor tissue can fool the immune system by expressing (or not express) surface proteins that mask the malignancy for the immune system.

\end{enumerate}
    
The two remaining hallmarks are "Tumor promotion inflammation" and "Genome instability and mutation"

\subsection*{Session 3 - Breast cancer}

\section*{Wednesday 16. Nov}

\subsection*{Session 1 - Introduction to imaging modalities}

\subsection*{Session 2 - Cell based therapies - Lab work and work-flow}

When developing new drugs, in a well functioning scientific community, strict regulations and documentation demand is an imperative.
This is by no means restricted to drugs involving chemicals, but reaching also cell-based therapy, where there exist a large amount of rules and regulations.
This regularities are in constant development as novel cellular therapies emerges.
Personel, equipment, safety, standard operating procedures and storage all have their specific regulations, ensuring a safe and robust research, development and production environment.

T-cell infiltration in tumor or the stroma has been identified (for quite some time ago) as prognostic factors for therapy end-points such as recurrence free survival, progression free survival and overall survival.
T-cell in the micro-enviroment of the tumors significantly stratified patient groups in Non-small cell Lung cancer cohort, time to half fraction disease specific survival seperated with as much as 70 months.
It is tempting then to do mRNA-profiling, identifying the immunologic signature, as it has been shown that surgery is less effective on survival with the immunsystem not involved - maybe pivoting to other treatments?
However, the antigen associated with these processes are largerly unknown, except for some melanoma-cases.

Profiling is one thing, another is active use of immunocells in cancer therapy.
A number of different therapies are currently being investigated, some in the Department of Cellular Therapy.

\subsection*{Session 3 - Patophysiology of exicatory diseases in the brain}

Glutamate is a highly interesting biological molecule, on one hand it is a basic and abundant neutransmitter, on the other hand a very potent neurotoxin.
The transmitting capabilities of glutamate was first described by Hayashy in 1954, when convolutions were observed when glutamate was injected into the brain or cartoid arteries of XX.
This physiological transmission of glutamate is imperative for normal function of the nerve system.
The glutamate is stores in vecicles in the synaptic synapse, are released into the region between the synapses and reaches receptors on the adjacent synapse.
The glutamate are then quickly cleared out, stored shortly in the astrocyte for then to be transported back into the synaptic nerve as glutamine.
Multiple glutamate receptors exist, and can be broadly categorized into three categories, AMPA-R, KAINATE-R and NMDA-R.
Further, NMDA-R has three subtypes, and the primary source of calcium influx.
The NMDA.-recetor consist of an N and C-terminal, and four M-groups on the membrane level.
The response of the receptor is a three-step process.
First, glutamat binds to the GluN2-unit, then GluN1 binds glycin, needed to open the Ca-channel.
The last step is relase of magnesium which when bound blocks the channel.
The relase of magnesium is facilitatet by AMPA-R receptors, and thus acts as a zeroth step for glutamate activation.
A remaining point on the NMDA-receptors is the connection with the PDZ-containing scaffold protein, the PSD-95 family proteins.
These proteins facilitate the insertion and anchoring of NMDA-receptors at synaptic sites, this plays a major role for the localization of them.

Exicitotoxicity is a pathological process involving the NMDA and AMPA-receptors.
The end-point of the process is severe neuron death.
These events can be triggered by a stroke, when nerve cells around an ischemic core relases a large amount of glutamate upon death.
The glutamate flows into the ischemic penumbrae, the area around the core that is not yet severely damaged.
The transport system that clears out the glutamate is hindred, and the result is a massive overactivation of glutamate, leading to an excess and un-interupted influx of calcium.
This leads in the next step to programmet cell death (apoptosis) and further brings damage to the organism.
The details of the pathways leading to cell death from the influx of calcium is debated.
They can be categorized into two main hypothesis:

\begin{enumerate}
\item
The threshold theory, involving several proteins in separate pathways, main actors being CASPASE, PARP1 and PSD95-NOS. The excess of calcium determines the excitotoxicity.
\item
The specific route theory: More debated, a precise and highly specific (but not neccesarily critically large) amount of calcium influx cause cell death
\end{enumerate}

\section*{Thursday 17. Nov}

\subsection*{Session 1 - Imaging cardiovascular diseases in animals and patients}

Imaging of muscle-cells during response is primarily done by measuring the calcium concentration in cytosol, giving an indication of muscle activity.
This is done by using so-called calcium-2+-flurophores, e.g. molecules that give of light when binding to calcium ions.
These calcium markers can work on two principles, one where the calcium-binding lead to an increase in fluorescence, the output can then be measured as increased intensity.
The second principle utilizes a shift in the fluorescence spectra when binding to calcium is present.
The fluorephores has to be transported into the cell membrane, two used techniques are 

\begin{enumerate}
\item
Using an ester-group bound to the fluorephores allowing passage through the membrane.
Then when transported through the membrane, esterase will cleave the ester-group off and the flurophores are in the cytosol.
\item
The second is more mechanic and straight forward - injection is done through a very small patch pipette.
\end{enumerate}

When detecting the calcium transition (calcium activity), several instruments are to our disposal: 
The relatively straight forward and cheap Wide-field imaging that can be used with both the single and double emission (intensity and spectrum shift) techniques. 
A major drawback of this technique is that the fluorescence is recorded throughout the sample in all depths, meaning that several regions will be out of focus.
Using confocal pinholes, the set-up is called confocal imaging and the planes out of focus can be "sorted out" and only the plane on focus can be recorded.
This allows a "depth" scan throughout the specimens and thus a 3D-reconstruction can be made.

Throughout the cardiac-cells are the T-tubulus, infodlings of the cell membranes.
In cells from patients with heart failures, these tubules are disrupted, leading to a damaged flux of calcium.
This disruption can be imaged by confocal microscopy, showing distinct pattern differences in non-healthy cells compared to normal cells.
An end-goal is to use this information and bring the description of the pathology from the cellular to an organ-model, currently being studied by a mathematical model compared to a mouse-heart model.

For imaging human patients in a clinical setting, the medical professional have multiple imaging modalities to his or hers disposal.
It is therefor a very important for the clinician to decide which image modality that best suits the current situation.
The initial examination, and also the most routinely done examination done on the heart in general, is an echocardiography.
Echocardiography, as in its name, means to write (graphein) the heart (cardio) with echoes (the ultrasound-technique).
The examination can be done by any health care professional with minimal training, is very little invasive and requires nothing more than an ultrasound apparatus.
The penetration depth of the ultrasounds is fairly short, but it is enough to take a good look at the heart.
An added benefit of the ultrasound, is that when equipped with Doppler-imaging capabilities, it is possible to quantify the velocity of the blood flow.
This information is color coded for direction (e.g. one color for left-right flow and another for right-left) and can measure the speed of blood flow, giving valuable information about the blood flow.
Pathology to look for is a dilation of the heart (the heart has increased in size to overcompensate for some underlying malignancy) or hypertrophy, a "thickening" of the heart.
The last pathology can also be strictly physiological, as it is associated with chronic exercise or pregnancy.
A key parameter apart from dilation and hypertrophy is ejection fraction, a measure of the fraction of blood in the left ventricle that gets ejected in systole.

The second most used, and much more invasive technique is coronary angiography.
It is based on contrast X-ray where a catheter with a contrast agent is inserted into the coronary artery, opened, and the flow is visualized with X-ray.
The procedure is quite painful for the patient.
When the echocardiography images the heart itself, the coronary angiography images the coronary artery, the large artery that transports oxygenated blood into the heart muscles.
The pathology encountered with this procedure are often obstructions of the artery, especially patients with ischemic heart disease. 

Other image-based modalities used on the heart are PET-scans (most often with FDG) and MRI-scans, described elsewhere.

\subsection*{Session 2 - Gut and liver inflammation and animal models in cancer}

\subsubsection{Gut and liver inflammation}

Our immune system is a complex system with numerous participants.
The picture becomes vastly more complicated when we also know that there is a close collaboration between the immune system and the microbiome.
The microbiota make out around ten to the fourteenth power of microbes.
This is a substantial fraction of the number of cells in the organism.
Add also to the complexity, one reckons there exist more than 1000 different species in the gut.
Although separated by a layer of epitelium, the immune system and the microbiota shapes each other, by exchange of molecules.
A concept introduced by the speaker is the "GxE"-concept, meaning genes multiplied with environment.
It highlights how the diversity of trait multiplies and adds up quickly, potentially a multitude of treatment related implications.
Also worth noted is that the genetics are more or less constant, but the environmental factors are highly versatile, and can be adjusted.

An example of the interplay between microbiota, immune system, genetics and the Environment is the inflammatory bowel disease, IBD.
IBD can be categorized into two main diseases, Chrohns disease and Ulcerative colitis.
Some findings indicates a strong hereditary factor in the development of IBD, but only 20 \% of the heritability is explained - meaning we are missing some part of the picture.
A single gene disorder has been identified, but the majority of IBD-cases is believed to involve a multitude of genes and environmental factors.
The environmental factor interplay has been shown by differences between populations separated by geography, and also an historical increase show that something clearly is going on.
This is further backed up with knock-out mice that when exposed to a germ free environment developed little to no inflammation, whereas in a "normal" environment they received extensive inflammation.
A wild type mouse did not show inflammation in any of the environments.

Due to the direct constant between the liver and gut, inflammation in the liver, coming from the gut has been observed.

\subsubsection{Animal models}

The talk was about (large) animals and their use in cancer research.
An interesting initial point was that in Norway, the vast majority of research animals are fish(!) due to our extensive salmon industry.
The need for larger, more human-like organisms (not rodents, zebra-fish or flatworms) are the physiological dissimilarities between these animals and humans.
They do not have to be at the tissue, or organ-level, a lot of cellular mechanisms are different from mice to humans as well.

One curious situation presented, was a largely American phenomenon, where the use of dogs - an animal very seldom used in Norway.
The dog is quite important, it was the subject for the first (successful) heart transplant, coronary bypass, heart-lung machine and other cardio-ciculatory procedures.
And, dogs get cancer.
Owners of pet dogs have a 20-30 \& chance for their pets to develop cancer, an illness previously not treated by veterinaries.
However, due to the similarities of cancer in dogs and their counterparts in humans, a large system for pet donation to drug-trials developed, fueled by the wish from pet-owners to treat mans best friend.
Dogs then can serve the pilot, pre-clinical phase and a drug can go directly from dog to phase one human trials, or in another case, a drug can go from first in dog, or first in human, through the dog before a phase 2 human trial, optimizing treatment knowledge.

The speaker introduced his own grouping of mice experiments, 12 in total.

\subsection*{Session 3 - Pancreatic beta-cells}

The talk was about a sophisticated transplant procedure helping to control diabetes.
The pancreas is an important organ in relation to numerous hormone regulation pathways, with the most known being the production of insulin.
When the pancreas no longer can regulate the insulin, or the mechanism for insulin response is failing, a disease called diabetes erupts.
This disease is a large, global problem.
In a Norwegian cohort study, one discovered that diabetes increases the mortality rate 3-5 times, and over 50 \% died due to acute and chronic complications (authors note: this should be worse in countries without proper health care).
The islets of Lagerhans are the insulin-producing cells in the pancreas.
Transplantation of these islets is an effective treatment of type 1 diabetes, with limited complications, when compared to insulin injections or full pancreas transplantation.
The transplant procedure consist of removing pancreas tissue from a donor, the tissue is hacked into small pieces and further mechanically purified, to separate the exocrone pancreatic tissue (98 \%, the tissue we do not need) from the remaining endocrine tissue.
These cells are then transfused into the recipients liver, where they reside.
The process is formalized through the so-called "Edmonton protocol" which was developed in the late 90s.
The outcome of this transplantation have increased in success throughout the years, and in a group treated between 2007 to 2010, 44 \& insulin independence was maintained 3 years after transplantation.
Major challenges for the therapy are a lack of donors and rejection of the islets due to immunologic reactions.
This challenges is difficult to address head on, as it has been shown that human and mice islets differ, initially preventing further studies of these islets in mice models.
But, a team in Oslo has made progress with a humanized mouse model, and it sounds very promising. 


\section*{Friday 18. Nov}

\subsection*{Session 1 - Biobanks}

The talk was about biobanks in general, with focus on Norwegian projects involving bio-banks.
Bio-banks is an emerging tool for the bio- and medical field.
In short, it is a large storage facility of biological samples, hopefully well categorized.
When enough meta-data is connected to the samples, they can provide the necessary amount of data to not only pure biological research, but also link environmental factors to find results concerning the vast and interconnected play between genetics and environmental factors.
This is true for both common diseases and rare genetic treats.
Fields that also have the use for large bio-banks are drug-development, and the study of bio-markers.

An important organization that brings the nordic biobanks together is Nordic Biobank Network, which includes biobanks in Sweden, Norway, Finland, Denmark, Iceland, the Farrow Islands and Estonia.
On a more national level there is the Biobank Norway, consisting of nine biobanks from Bergen to Tromsø, coordinated by NTNU.
The Norwegian biobanks (authors note: are we bragging to much?) are characterized by high quality in all levels (QA/QC/sample quality/personnel and so on).
There are several reason for biobanks to be a success in nordic countries, a research friendly population, health registries, but a key element is the personal identifier, a unique number attributed to each individual, making cross referencing between registries possible.
One of (even though there are several) the largest and most known bio-bank-related population studies are the HUNT (Helseundersøkelsen i Sør-Trøndelag) studies, with HUNT4 starting autumn 2017.
The study is vast with 220 000 screened with 126 000 of them screened with questionaire data, clinical examinations and blood sampling.
In this group, over 8000 variables are store in the databank, making it a huge and unique project.
The HUNT biobank store plasma, DNA, immortalized cells and other tissue samples in low temperature and automated storage facilities.
Some results from the HUNT2 (95-97) have been linked to the Cancer Registry to show incidence of Cancer.

When this large data material is present, it is possible to conduct research concerning precision medicine, where the genetic mark-up of the individual.
More advanced (and also alot cheaper) techniques for sequencing DNA allow us to conduct such research, which were expanded upon in other talks.
It is clear that bio-banks provide an essential part in this research.

\subsection*{Session 2 - Imaging diseases}


\section*{Monday 21. Nov}

\subsection*{Session 1 - Personalized medicine}

\subsubsection{Personalized medicine}

The talk was an overview of the concept personalized medicine.
Personalized medicine may be a bit of a misnomer, better words (and currently synonyms) could be stratified medicine, precision medicine or tailored medicine.
Even though it is a reltively novel field of research, the idea and concept is not new at all, as variability in patients even inside the "classical groups" (e.g. patients sharing a common disease) large individual differences can be found, and it is not far fetched to think that these differences affect the treatment outcome.
The idea got an initial large traction by the turn of the century, when the HUGO project managed to map out the total genomic profile for a single human individual.
This was a major breakthrough for genetics, and sparked an immense optimism.
However, the initial optimism was curbed by one major biological component, the expression of genes which is not uniqucly identified by the genome, and a financial, sequencing the genome was immencely expensive.
But, this optimism is on its way back, as increased understanding of epi-genetics, developmentents in biological and hardware tools and an increasing amount of stored data will help us understand complex system we did not grasp 15 years ago.
This opens a range of possibilities for screening for treatment sensitivity, disease burden and other indicators we can mtitgate with more stream-lined treatment.

However, this optimism comes with a cost.
When we are stratifying smaller and smaller populations, we loose a certain power from the evidence-based, the control group.
The gold standard for treatment development is randomized double blind trials, and with personlized medicine with one patient, one group and one disease, do not give room for a control.
This is maybe the hardest challenge to overcome, as it has to be met with a reshaping of how we look at the "best" way of developing new therapies.

\subsubsection{Tumor Immune Evasion Mechanism}

The second part of the talk was a look at how tumors evade the immune system.
One of the "revised" hallmarks of cancer, is avoiding immune destruction.
Several talks have expanded upon the subject, but this talk gave a nice general overview.
One of the earliest indications of the tumor-immuno-relationship was described in the middle of the 1800s by Rudof Virchow that noticed leucocytes in tumor tissue.
This lead to his hypothesis that chronic inflammation and cancer were strongly linked 

\subsection*{Session 2 - Drug sensitivity in leukemia}

\subsection*{Session 3 - T-cell therapies in cancer and cancer metabolism}

\section*{Tuesday 22. Nov}
\subsection*{Session 1 - Medical chemistry and drug development}
\subsection*{Session 2 - Imaging disease - multiphoton imaging}

The talk was a highly interesting round-up of multi photon imaging, and have they have given insight in the world of the astrocytes.
The astrocytes are the "other" brain tissue, making up a large part of the brain comparative in the number of cells to the neurons.
They were discovered and described by Cajal in the 1900s, however, since they did not respond to electrical stimuli, it was difficult to study them directly.
More recent discoveries with sophisticated electron microscopy revealed that they contained an overwhelmingly number of aquaporins, AQO4.
These porins are proteins that facilitate rapid and selectively, not passively, transport of water molecules.

Around the glial membrane cells, adjacent to vessels, nearly 50 \% of the area is containing these proteins.
Interestingly, these porins seems to worsen the damage of brain edemas and stroke, begging the question why they are present in our brain?
One tool to investigate this is the two-photon laser scanner.
With it, fluorescence can be detected with micrometer precision and with a temporal resolution of a few milliseconds.

Extra attractive is the fact that it can be done on live mice brains, ether with a temporary coverglass with a part of the cranium removed, directly over a thinned cranium, or as a chronic window, where a cover-gass is attached chronicly to an opening in the skull.
Through fluorophores, encountered and described in relationship with the calcium imaging, the flow of fluid in the brain can be monitored in practically real time.
With this technique, it was shown that there exist a "flushing" of cerebral spinal fluid through the brain, facilitated by the aquaporins.
This is believed to work in analogue to the lymphatic system, as the brain lack lymph nodes and need a process to get rid of toxic waste products.
The route of this transport is debated, on one side there could be a convective flow, pushing the waste into vessels.
Another theory relies on the capillary network, as the distance traversed are more in line with the time frame.

The second application of the two-photon microscope introduced, was tracking a depressing wave through the brain.
This depressing wave, where the mystery was what mediators was in charge of this wave.
With the two-photon microscope, it was possible to spatially and temporally track the mediators, and a slight time-shift was found between them, shedding light on the process.




\subsection*{Session 3 - Predictive medicine, epidomology}
\section*{Wednesday 23. Nov}
\subsection*{Session 1 - Venous thrombosis}

The presentation was about venous thrombosis, the disease, the pathology and the risk factor associated with it.
Venous thrombosis happens quite often in the population, with 1-2 incidents per 1000 people.
Mortality is associated with a 30 day case-fatality rate of 5-10 \%.
The common type is the deep vein thrombosis(DVT, 2/3 of the cases) which originates in the vessels of the extremities.
The more serious, Pulmonary embolism (PE) accounts for the remaining third of cases.
It is assumed that the incidence of VTE will increase over the years, leading interest in identification in risk factors and potentially mitigate the problem through treatment.

There are believed to be three major conditions that has to be present for VTE to develop.
A vessel wall injury, which can stem from degradation due to aging, surgery similar damage.
Stasis, caused by obesity, pregnancy, still-life due to surgery or general immobility.
The last one is hypercogulability, which can come from obesity, pregnancy, hormone disturbance or thrombophilia.
On the more microscopic scene, a leading hypothesis suggests that Monocytes and other immunocells get trapped in vortexes cause by the flow through the valvular sinuses.
These vortexes also cause an intermedittant hypoxia.
The monocytes adhere to the endothelium and causes build-up of fibrin.

We know some things of risk factors in VTE.
A first and striking point is that the risk increases dramatically if the patient is hospitalized.
There are also a larger incidence rate in age, with the increase starting at 30-40 years old.
As obesity is a condition related to both stasis and hypercogulability, it should be an indicator for VTE as well.
However, it is somewhat complicated as the exact mechanism is unknown - it could either be pressure on the body from excess mass, or a more linked interaction between metabolic dysfunction and molecular pathways.
Some genetic risk factors has been identified, but the whole picture is not known as there exist a gap between observations done in familiy and twin studies and genetic risk factors.
This urges us to investigate the underlying genetic risk factors
A hypothesis presented was a "potential model" where several factors were needed to push the risk up and above a level which induced VTE.

VTE risk factors can be found together with risk factors for connected diseases.
Especially cardiovascular diseases are interesting as thee diseases share some risk fators.
There has been found a correlation between VTE and myocardial infarction, especially in the time after the infarction.
The same association can be found with stroke, however only in a short period after the stroke.
It remains an unanswered question if there are strictly common risk factors leading to both VTE and arterial thrombosis, or if risk factors lead to arterial thrombosis which in turn lead to venous thrombosis.
The two other co-morbidities mentioned were cancer and hospitalization.
For cancer, an increased risk was found both after and before the incidence of cancer.
As both before and after cancer increases risk, the mechanism is not limited to the treatment, however, treatment of cancer is a intuitive risk factor.
The connection between cancer and VTE have lead to simple prediction scheme, where a score for the risk of VTE is given an the basis of cancer site, blood values and BMI (the Khorana Risk Score).
A similar risk score is found for hospitalization, with the basis of previous complications.

\subsection*{Session 2 - DNA repair and epi genetics}

\subsubsection{DNA damage and repair}

The talk was about DNA-damage and repair, describing the different ways DNA can be damaged, repaired, and the consequences it has for the genomic integrity.
The DNA is not a specially stable molecule.
It is constantly under "attack" from radiation, erroneous replication, secondary damage from radicals and other dangers.
There are however, numerous repair mechanisms for when things go wrong.
The way DNA is repaired is strongly linked to the type of damage.
The vast majority are structurally damages to the DNA-helix through chemical alterations of the bases, inducing bulky lesions from non-native chemical bonds.
The DNA can be repaired "directly" through photolyses, alkyltransferase or ligase processes.
Excision repair is needed in the 

\subsubsection{Epi genetics}


\subsection*{Session 3 - Cardiovascular disease}

\subsubsection{In animals and humans}

The talk was an introduction to the use of animal models in heart-diseases, first a general introduction and some examples with mice.
Heart failure affects a large part of the earths population, with 2-3 \% of the population affected.
Ischemia, developed from atherosclerosis is a major cause of death in the first world.
Clinical outcome of heart failure has came with the use various drugs, but it remains a "big killer".
This is largely due to somewhat poor understanding of the underlying mechanisms.

Research on human has its limitation, thus we need good animal models to emulate the human organism.
A lot of prior success-histories in heart treatment has come from the dog, pig and rodents like rats, mice and rabbits.
Of course, animal models comes with some caveats, no animal model is perfect and differences from human physiology arise quickly.
Apart from biological challenges, one faces also legal, ethic and cost-challenges, all of which has to be considered.
A little conundrum is also that the more human-like our research subjects become, the more ethical problems arises.

Due to the similar heart physiology, pathophysiology, genetic make-up and short life-span, the mouse and rat are favorable animals for studying the heart.
These rodents can be used as test subjects for surgical intervention, mimicking heart operations or induce acute heart failurees by blocking vessels.
Intervention in the form of pharmacological remedies to emulate heart failures is another approach, or introducing infection agents to trigger autoimmune reactions.
Putting mice or rats on a strict diet can also help to understand how life-styles including high salt, sugar or fat can affect the heart.
A still growing library of knock-out mice also serve as helpers to understand the heart.
It allow us also to study "clean" models where we can have geneticaly obese and diabetic individuals without hypertension.

Other animals as non-human primates, the zebra fish and the fruit fly are also of course used for medical research.
The primate, although seldom used do to cost, handling and ethical concerns, has helped our understanding of drug dynamics and heart failure related to HIV.
The zebra-fish and fruit fly, as expanded elsewhere in this report, gives good genetic tools.

Hearts with heart failures have generally undergone some kind of remodeling, either a dilation or hypertrophic changes.
These restructuring have been linked to the extracellular matrix, surrounding the heart cells.
Some interesting proteins, the proteoglycans, have been found to be related to hypertrophy, one study found Syndecan-4 knock outs to not develop concentric hypertrophy.
This could indicate that these proteins can be used as potential bio-markers.
Some interesting bio-markers found 

\section*{Thursday 24. Nov}

\subsection*{Session 1 - Structural biology}

The talk was about structural protein discovery, exemplified through work on APT-ase
X-ray crystallography is an imaging technique that can, quite briefly said, be used to image molecular structures.
When it was discovered in the 30s that proteins, when purified, also did diffract X-rays in a well behaved manner, it opened up the possibility to image the structure of proteins.
Proteins are large structure, containing a multitude of atoms, and further numerous sub-parts making up a complex geometry.
This complex geometry is necessary for the protein-function, and therefore it is crucial, and highly interesting, to image the protein structure.
Due to the geometric complexity, the first protein was not fully described before 1960.
Now, with the advent of computationally  powerful hardware, it is possible to study more and larger proteins.
Of course, there are many other difficulties than the mathematically calculation of the structure from the diffraction pattern.
Before the X-ray itself, the protein has to be biologically identified, produced in a relatively large amount.
It then has to be purified, crystallized and be ensured to maintained its crystallized structure.

After the crystallization, the imaging itself is highly technologically difficult, using large facilities that produce highly collimated, single wavelenght X-rays.
The crystals has to be rotated to produce the diffraction pattern.
The diffraction pattern are then used to calculate and visualize the electron density, this could be considered the "raw data" in crystallography.
Now, a model calculation is performed to transfer this electron density into a final structure.
In this step, there is a clear distinction from the previous "image and calculate" to "calculate and interpret" by using different chemical and mathematical model to work out the underlying structure.


\subsection*{Session 2 - Colorectal cancer}

The talk gave insight in Colorectal cancer, and why it is a favorable disease for the development of bio-markers for.
Colorectal cancer is a disease that affects a large number of patients, and have a high mortality.
There are some geographical spread, and it looks to affect mainly people in the first world, with a steadily increasing incidence rates in the nordic countries.
It is also expensive to manage, relying on expensive procedures, and the mortality increases sharply with illness progression.
If colorectal cancer is discovered early however, it is highly manageable and this "window of opportunity" from illness to fatal progression is quite wide, spanning years and sometimes decades.
All these factors make colorectal cancer a suitable candidate to develop screening programs for.
The "gold standard" for colorectal cancer screening is endoscopy, either in the form of a colonoscopy (the whole colon) or a sigmoidoscopy (only the lower part of the colon).
There is a large screening project currently being explored involving endoscopy, however, the procedure is quite costly and invasive.
A less invasive procedure is the stool sample, the Fecal occult blood test, being performed in a selection of european countries.

The attention is then directed to viable biomarkers, which can give an indication for malignancy in the patient sample.
The marker has to have a high degree of specificity, meaning it cant be present in normal tissue, and sensitivity, present in the majority of tumor tissue.
A group of biomarkers presented is methylation of certain parts of the DNA.
The methylation of DNA in cancers are highly abundant, more frequent than mutations, and can be identified early in the disease development.
This makes the monitoring of methylation a viable and interesting biomarker.
As of now, six distinct biomarkers has been identified, and methodologies to detect them has been developed.
All six performs well, with a panel having a sensitivity of adenoma and carcinoma of 93 and 94 \% respectively, with a 98 \% specificity compared to normal mucosa.
This means that although the test will give some positives in the form of adenomas, it will detect the majority of carcinomas with very few false positives.
This is very interesting as it utilizes epigenics for screening, with a high degree of accuracy in a troublesome disease.

\subsection*{Session 3 - Away}
\section*{Friday 18. Nov}

\subsection*{Session 1 - Beta-cells - scrap}

T1D and T2D are complex diseases depending on a lot of factors - chronic conditions, lifestyle, in utero develepment, parental obesity (recent in nature), environmental factors, nutrions, intertestional flora, aging (espicielly type 2)
The human is a bad subject, long lifespan, ethical problems, legal issues and not many volunteers.
In addition it is long generation time and small number of offsprings.
No possibility of genetic manipulation, and highly variable background and variable life-style.
Humans have bad dicipline.

Alternative is to go in vitro.
Take iPSC, take different conscription factors and end up with a mature $\beta$-cell (7 stages)
Possible to take the last steps (4-7) into a mouse model.
Purity is a problem, along with: it is not systemic, it is inefficient, inaccurate and the output is very premature, we need more tools!

Of course, the situation is complex, and no perfect tool is present.
Different murine systems:

\begin{enumerate}
    \item
	Ob/Ob mice - mutation of leptin, they gain weight rapidly and develop hyperglycemia, this is a T2D-model.
    \item
	Db/Db mice - mutation of leptin receptor, more severe symptoms than Ob/Ob, this is also a T2D-model
    \item
	Akita mice - mutation of insulin2 - note obese, selective toxicity depletion of beta-cells. Develops insulin dependent diabetes - resembles very much a T1D-case and is therefor used 
\end{enumerate}

Chemically induced abation - streptozotocine and alloxan - inject and wait for beta-cells to be depleted.
Alot of problems, it is toxic, unspecific, inefficient and inaccurate.

Transgenic models, harder and more expensive.
Have a genome, add a transgene, specific or random. 
Use a specific promoter that trigger transcription. 
Then inject into a host-genome

Transgenic and toxin induced ablation - Diptheria toxin - and a DTA-system where you kill 75 \% of $\beta$-cells.
Downsides: Patial and variable ablation efficiency and requires inducible systems.
Use a promoter and mark a cell for killing - kill exceptionally specific and efficent (99\% $\beta$-cell loss).
The downside of the promotor-method is that you need to reate a transfenic line for each promoter.

The third model: Transgenic and specific gene targeting.

Need to trace the cells, remove the stop-codon before the gene for a green fluorescence.
Problem: Cells are irreversible traced from the moment they express the promoter.
Can have two identical cells of unknown origin.
We can use conditional cell-tracing to separate them from eachother.
With this we can identify apparently identical processes, it is highly specific...

Back to the diabetes:
We can observe the outcome of the cell:

\begin{enumerate}
	\item
	    We can have decreased proliferation
	\item
	    Dedifferentiation
	\item
	    Transdifferation - express second hormons
	\item
	    No label observed - death
\end{enumerate}

We can use cycline to kill the cells.
Beta-cells not really dying but stop to produce insulin (real reason for diabetes)

(lots of stuff)

Things to remember:

The 2 most prevaltnt types of Diabetes are complex disorrders and hard to charectirize (esp. in humans)

There are many models (in vitro, spontaneus, chemical and transgenic)

Cell death is not the only mechanism of beta-cell decay, dedifferation, preliferation, transdifferation etc.

Genetic cell tracing esp. inducible systems are required to properly analyse cell fate

\emph{Regenerative strategies} - proliferation, trans and redifferantion and neogenesis
Different tissues can do one or several, or all.

The innate and spontaneus B-cell regenerative
Loss of 50-70 \% self-renoval by cell division, in extreme cases (99\% loss, conversion)

The best is cell-division - as it is fast.
Obecity and pregancy show increase in beta-cell mass.
Beta-cell is able to increase proliferation upon stress and loss (due to for example injury).
Chemical drugs can also induce proliferation, Glucose, CLP1 and S961 (this is specific and a dose dependent effect have been shown, human cells do not react to this drug).

The second scenario is when you have almost no beta-cells left.
The pancreatic islet have different cells, alpha, beta, PP cells and delta-cells.
RIP-DTR ablation system to model T1D, killing 99\% of the beta-cells, leading immidiantly to diabetes. 
The mice can regerate up to 70\% of the normal amount in adults.
Use alpha-tracking show that the beta-like cells started as alpha-cells. 
They stopped to express glucagon and started expressing insulin (hurray!)
Not in humans (?)

Cell interconversion also occurs after partial loss.
Interconversion can appear in humans (in T2D not T1D)

Summary: The dynamic of b-cell decay failure, alter the choice of the 

\begin{enumerate}
    \item
	Human B-cell proliferate slower
    \item
	A minimum number is required for preliferation based strategies
    \item
	Some B-cell proliferate drugs were promising in mice 
    \item
	Total absence will trigger other mechanisms
    \item
\end{enumerate}


\subsection*{Session 2 - At the interface of chemical kinetics and population biology}

Post-atibiotic era: No new antibiotics (no new classes, classes have a common target and similar chemical structure).
These classes often have similar resistenca mechanisms - resistance is a global problem that is being achnowledged.
Talk of two diseases, Tuberculosis (big killer, global problem) and hospital acquired infections.
Large drop in TB the last 100 years in europe.
Pasteurisation did help as a bovine variety was dangerous.

This is not to be seen in resource-poor countries. 
Strongly resembles HIV-prevalence. 
5-10 \% is infected but not sick, but with HIV you develop disease and starts to spread disease.
HIV and TB act together.

TB is a very underestimated disease - underestimated by 2-folds by the WHO.
Biggest problem is multi-drug rsistant tubercolosis - worst in eastern europe. 

Death toll in europa for HA-infections os much larger than multi-drug-resistance and HIV-infections (comparable to breast cancer).
8 \% of hospital stays result in infection, of which half are multi-resistant.
Treatment right away is very important.

Erlich: Hit hard and early, little to no eveidence for this but we still do it.
Important questions: 

\begin{enumerate}
    \item
	How often?
    \item
	How much?
    \item
	How long?
\end{enumerate}

Problem to coonect the different levels of interaction.
A simple model of bacterial killing: Look at the slides

Set up an experiment to monitor replication in a single cell.
THere is some kind of "lag", even after the drug has stopped being present, the bacterial wait a bit to start replicate again)
There is a reservoir of unspecifically bound antibiotics.
Antibiotics need to "leave" the cell.
Explosive onset of growth: antibiotics "diluted" by replication.
This can be used to explain how often one should give antibiotics.

End-goal of their work is to stream-line drug-developement.

\subsubsection{Systems biology}

Combine the whole gene, protein and chemistry parts into a whole system.
Two typical work-flow is to gather data, integrate and make into a correlative model so we can predict outcomes.

Cancer evolves when the system get perturbed, as cancer is an error on the genetic level.

Some causes, and effect - this can be translated to an interaction network.

Now possible to sequence whole genom quite cheap.

Also look at regulatory gene-coding, not just protein-coding.

Trancriptions and proteomes are the key to underestand how disruption of this can cause cancer.





\end{document}

